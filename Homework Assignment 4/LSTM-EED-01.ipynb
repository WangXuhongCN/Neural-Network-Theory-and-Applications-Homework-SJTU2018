{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical,np_utils\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, LSTM, Activation,Dropout,Masking, Embedding\n",
    "from keras import optimizers\n",
    "from  keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.image as pimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(self,n_epochs):\n",
    "    #iters = range(len(self.epoch))\n",
    "    iters = range(n_epochs)\n",
    "    plt.figure()\n",
    "    # acc\n",
    "    plt.plot(iters, self.history['acc'] [:n_epochs],'r', label='train acc', linewidth=1.0)\n",
    "    # loss\n",
    "    plt.plot(iters, self.history['loss'][:n_epochs], 'g', label='train loss', linewidth=1.0)\n",
    "    \n",
    "    # val_acc\n",
    "    plt.plot(iters, self.history['val_acc'][:n_epochs], 'b', label='val acc', linewidth=1.0)\n",
    "    # val_loss\n",
    "    plt.plot(iters, self.history['val_loss'][:n_epochs], 'k', label='val loss', linewidth=1.0)\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0,1.2)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best',shadow=True, fontsize='6')#loc=\"upper right\"\n",
    "    plt.savefig('loss of lstm 01.png', dpi=400)\n",
    "    sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_data_01=np.load('D:\\\\GitHub\\\\Neural-Network-Theory-and-Applications-Homework-SJTU2018\\\\Homework Assignment 4\\\\data_used\\\\01.npz')\n",
    "files_in_zip_01 = zip_data_01.keys()\n",
    "label01= np.load('D:\\\\GitHub\\\\Neural-Network-Theory-and-Applications-Homework-SJTU2018\\\\Homework Assignment 4\\\\data_used\\\\label.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一个cell：数据预处理，将01.npz数据集变成（15,265,310）的形式，长度不够265的补零\n",
    "也就是说，将变长序列变为定长序列，\n",
    "变形方法：https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros \n",
    "行为解释：https://www.cnblogs.com/leeshum/p/6089286.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01=np.zeros((15,265,310))\n",
    "for i in range(15):\n",
    "    for j in range(265):\n",
    "        if j < zip_data_01[files_in_zip_01[i]].shape[1]:\n",
    "            data01[i,j,:]=zip_data_01[files_in_zip_01[i]][:,j,:].reshape(310)\n",
    "        else:\n",
    "            data01[i,j,:]=np.zeros(310)\n",
    "data01/=np.max(data01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=data01[:9,:,:]\n",
    "x_test=data01[9:,:,:]\n",
    "y_train=label01[:9]\n",
    "y_test=label01[9:]\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 265, 310)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 265, 128)          224768    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 356,739\n",
      "Trainable params: 356,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "    #model.add(Embedding(81250, 128, mask_zero=True))\n",
    "model.add(Masking(mask_value=0,input_shape=(265, 310)))\n",
    "model.add(LSTM(128,return_sequences=True))#\n",
    "#model.add(Activation('relu'))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "RMS = optimizers.RMSprop(lr=1e-5)\n",
    "model.compile(optimizer=RMS,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"model-1-best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "earlystop=EarlyStopping(monitor='val_loss', patience=20, verbose=2, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a model\n",
    "model=load_model('D:/GitHub/Neural-Network-Theory-and-Applications-Homework-SJTU2018/Homework Assignment 4/model-01-best.hdf5')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ------------\n",
      "Train on 9 samples, validate on 6 samples\n",
      "Epoch 1/500\n",
      "Epoch 00001: val_loss improved from inf to 1.13825, saving model to model-1-best.hdf5\n",
      " - 6s - loss: 1.1474 - acc: 0.3333 - val_loss: 1.1382 - val_acc: 0.3333\n",
      "Epoch 2/500\n",
      "Epoch 00002: val_loss improved from 1.13825 to 1.13306, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1390 - acc: 0.3333 - val_loss: 1.1331 - val_acc: 0.3333\n",
      "Epoch 3/500\n",
      "Epoch 00003: val_loss improved from 1.13306 to 1.12912, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1337 - acc: 0.3333 - val_loss: 1.1291 - val_acc: 0.3333\n",
      "Epoch 4/500\n",
      "Epoch 00004: val_loss improved from 1.12912 to 1.12590, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1296 - acc: 0.3333 - val_loss: 1.1259 - val_acc: 0.3333\n",
      "Epoch 5/500\n",
      "Epoch 00005: val_loss improved from 1.12590 to 1.12317, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1263 - acc: 0.3333 - val_loss: 1.1232 - val_acc: 0.3333\n",
      "Epoch 6/500\n",
      "Epoch 00006: val_loss improved from 1.12317 to 1.12079, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1235 - acc: 0.3333 - val_loss: 1.1208 - val_acc: 0.3333\n",
      "Epoch 7/500\n",
      "Epoch 00007: val_loss improved from 1.12079 to 1.11868, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1210 - acc: 0.3333 - val_loss: 1.1187 - val_acc: 0.3333\n",
      "Epoch 8/500\n",
      "Epoch 00008: val_loss improved from 1.11868 to 1.11679, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1188 - acc: 0.3333 - val_loss: 1.1168 - val_acc: 0.3333\n",
      "Epoch 9/500\n",
      "Epoch 00009: val_loss improved from 1.11679 to 1.11508, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1169 - acc: 0.3333 - val_loss: 1.1151 - val_acc: 0.3333\n",
      "Epoch 10/500\n",
      "Epoch 00010: val_loss improved from 1.11508 to 1.11353, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1151 - acc: 0.3333 - val_loss: 1.1135 - val_acc: 0.3333\n",
      "Epoch 11/500\n",
      "Epoch 00011: val_loss improved from 1.11353 to 1.11212, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1135 - acc: 0.3333 - val_loss: 1.1121 - val_acc: 0.3333\n",
      "Epoch 12/500\n",
      "Epoch 00012: val_loss improved from 1.11212 to 1.11082, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1120 - acc: 0.3333 - val_loss: 1.1108 - val_acc: 0.3333\n",
      "Epoch 13/500\n",
      "Epoch 00013: val_loss improved from 1.11082 to 1.10963, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1107 - acc: 0.3333 - val_loss: 1.1096 - val_acc: 0.3333\n",
      "Epoch 14/500\n",
      "Epoch 00014: val_loss improved from 1.10963 to 1.10853, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1094 - acc: 0.3333 - val_loss: 1.1085 - val_acc: 0.3333\n",
      "Epoch 15/500\n",
      "Epoch 00015: val_loss improved from 1.10853 to 1.10752, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1083 - acc: 0.3333 - val_loss: 1.1075 - val_acc: 0.3333\n",
      "Epoch 16/500\n",
      "Epoch 00016: val_loss improved from 1.10752 to 1.10660, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1072 - acc: 0.3333 - val_loss: 1.1066 - val_acc: 0.3333\n",
      "Epoch 17/500\n",
      "Epoch 00017: val_loss improved from 1.10660 to 1.10574, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1062 - acc: 0.3333 - val_loss: 1.1057 - val_acc: 0.3333\n",
      "Epoch 18/500\n",
      "Epoch 00018: val_loss improved from 1.10574 to 1.10495, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1053 - acc: 0.3333 - val_loss: 1.1050 - val_acc: 0.3333\n",
      "Epoch 19/500\n",
      "Epoch 00019: val_loss improved from 1.10495 to 1.10423, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1045 - acc: 0.3333 - val_loss: 1.1042 - val_acc: 0.3333\n",
      "Epoch 20/500\n",
      "Epoch 00020: val_loss improved from 1.10423 to 1.10356, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1037 - acc: 0.3333 - val_loss: 1.1036 - val_acc: 0.3333\n",
      "Epoch 21/500\n",
      "Epoch 00021: val_loss improved from 1.10356 to 1.10294, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1030 - acc: 0.3333 - val_loss: 1.1029 - val_acc: 0.3333\n",
      "Epoch 22/500\n",
      "Epoch 00022: val_loss improved from 1.10294 to 1.10238, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1023 - acc: 0.3333 - val_loss: 1.1024 - val_acc: 0.3333\n",
      "Epoch 23/500\n",
      "Epoch 00023: val_loss improved from 1.10238 to 1.10186, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1017 - acc: 0.3333 - val_loss: 1.1019 - val_acc: 0.3333\n",
      "Epoch 24/500\n",
      "Epoch 00024: val_loss improved from 1.10186 to 1.10139, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1012 - acc: 0.3333 - val_loss: 1.1014 - val_acc: 0.3333\n",
      "Epoch 25/500\n",
      "Epoch 00025: val_loss improved from 1.10139 to 1.10096, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1007 - acc: 0.3333 - val_loss: 1.1010 - val_acc: 0.3333\n",
      "Epoch 26/500\n",
      "Epoch 00026: val_loss improved from 1.10096 to 1.10057, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.1002 - acc: 0.3333 - val_loss: 1.1006 - val_acc: 0.3333\n",
      "Epoch 27/500\n",
      "Epoch 00027: val_loss improved from 1.10057 to 1.10021, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0997 - acc: 0.3333 - val_loss: 1.1002 - val_acc: 0.3333\n",
      "Epoch 28/500\n",
      "Epoch 00028: val_loss improved from 1.10021 to 1.09988, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0993 - acc: 0.3333 - val_loss: 1.0999 - val_acc: 0.3333\n",
      "Epoch 29/500\n",
      "Epoch 00029: val_loss improved from 1.09988 to 1.09958, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0990 - acc: 0.3333 - val_loss: 1.0996 - val_acc: 0.3333\n",
      "Epoch 30/500\n",
      "Epoch 00030: val_loss improved from 1.09958 to 1.09931, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0986 - acc: 0.3333 - val_loss: 1.0993 - val_acc: 0.3333\n",
      "Epoch 31/500\n",
      "Epoch 00031: val_loss improved from 1.09931 to 1.09907, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0983 - acc: 0.3333 - val_loss: 1.0991 - val_acc: 0.3333\n",
      "Epoch 32/500\n",
      "Epoch 00032: val_loss improved from 1.09907 to 1.09885, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0981 - acc: 0.3333 - val_loss: 1.0988 - val_acc: 0.3333\n",
      "Epoch 33/500\n",
      "Epoch 00033: val_loss improved from 1.09885 to 1.09864, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0978 - acc: 0.3333 - val_loss: 1.0986 - val_acc: 0.3333\n",
      "Epoch 34/500\n",
      "Epoch 00034: val_loss improved from 1.09864 to 1.09846, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0976 - acc: 0.3333 - val_loss: 1.0985 - val_acc: 0.3333\n",
      "Epoch 35/500\n",
      "Epoch 00035: val_loss improved from 1.09846 to 1.09829, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0973 - acc: 0.3333 - val_loss: 1.0983 - val_acc: 0.3333\n",
      "Epoch 36/500\n",
      "Epoch 00036: val_loss improved from 1.09829 to 1.09813, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0971 - acc: 0.3333 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 37/500\n",
      "Epoch 00037: val_loss improved from 1.09813 to 1.09799, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0969 - acc: 0.3333 - val_loss: 1.0980 - val_acc: 0.3333\n",
      "Epoch 38/500\n",
      "Epoch 00038: val_loss improved from 1.09799 to 1.09786, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0968 - acc: 0.3333 - val_loss: 1.0979 - val_acc: 0.3333\n",
      "Epoch 39/500\n",
      "Epoch 00039: val_loss improved from 1.09786 to 1.09773, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0966 - acc: 0.3333 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 40/500\n",
      "Epoch 00040: val_loss improved from 1.09773 to 1.09761, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0964 - acc: 0.3333 - val_loss: 1.0976 - val_acc: 0.3333\n",
      "Epoch 41/500\n",
      "Epoch 00041: val_loss improved from 1.09761 to 1.09750, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0963 - acc: 0.3333 - val_loss: 1.0975 - val_acc: 0.3333\n",
      "Epoch 42/500\n",
      "Epoch 00042: val_loss improved from 1.09750 to 1.09739, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0961 - acc: 0.3333 - val_loss: 1.0974 - val_acc: 0.3333\n",
      "Epoch 43/500\n",
      "Epoch 00043: val_loss improved from 1.09739 to 1.09728, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0960 - acc: 0.3333 - val_loss: 1.0973 - val_acc: 0.3333\n",
      "Epoch 44/500\n",
      "Epoch 00044: val_loss improved from 1.09728 to 1.09717, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0958 - acc: 0.3333 - val_loss: 1.0972 - val_acc: 0.5000\n",
      "Epoch 45/500\n",
      "Epoch 00045: val_loss improved from 1.09717 to 1.09707, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0957 - acc: 0.3333 - val_loss: 1.0971 - val_acc: 0.5000\n",
      "Epoch 46/500\n",
      "Epoch 00046: val_loss improved from 1.09707 to 1.09697, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0956 - acc: 0.5556 - val_loss: 1.0970 - val_acc: 0.6667\n",
      "Epoch 47/500\n",
      "Epoch 00047: val_loss improved from 1.09697 to 1.09686, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0954 - acc: 0.5556 - val_loss: 1.0969 - val_acc: 0.6667\n",
      "Epoch 48/500\n",
      "Epoch 00048: val_loss improved from 1.09686 to 1.09676, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0953 - acc: 0.5556 - val_loss: 1.0968 - val_acc: 0.5000\n",
      "Epoch 49/500\n",
      "Epoch 00049: val_loss improved from 1.09676 to 1.09665, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0951 - acc: 0.5556 - val_loss: 1.0967 - val_acc: 0.5000\n",
      "Epoch 50/500\n",
      "Epoch 00050: val_loss improved from 1.09665 to 1.09655, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0950 - acc: 0.5556 - val_loss: 1.0965 - val_acc: 0.5000\n",
      "Epoch 51/500\n",
      "Epoch 00051: val_loss improved from 1.09655 to 1.09644, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0949 - acc: 0.5556 - val_loss: 1.0964 - val_acc: 0.5000\n",
      "Epoch 52/500\n",
      "Epoch 00052: val_loss improved from 1.09644 to 1.09633, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0947 - acc: 0.5556 - val_loss: 1.0963 - val_acc: 0.3333\n",
      "Epoch 53/500\n",
      "Epoch 00053: val_loss improved from 1.09633 to 1.09621, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0946 - acc: 0.5556 - val_loss: 1.0962 - val_acc: 0.3333\n",
      "Epoch 54/500\n",
      "Epoch 00054: val_loss improved from 1.09621 to 1.09610, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0944 - acc: 0.5556 - val_loss: 1.0961 - val_acc: 0.3333\n",
      "Epoch 55/500\n",
      "Epoch 00055: val_loss improved from 1.09610 to 1.09598, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0943 - acc: 0.5556 - val_loss: 1.0960 - val_acc: 0.3333\n",
      "Epoch 56/500\n",
      "Epoch 00056: val_loss improved from 1.09598 to 1.09586, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0941 - acc: 0.5556 - val_loss: 1.0959 - val_acc: 0.3333\n",
      "Epoch 57/500\n",
      "Epoch 00057: val_loss improved from 1.09586 to 1.09574, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0940 - acc: 0.5556 - val_loss: 1.0957 - val_acc: 0.3333\n",
      "Epoch 58/500\n",
      "Epoch 00058: val_loss improved from 1.09574 to 1.09562, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0938 - acc: 0.5556 - val_loss: 1.0956 - val_acc: 0.3333\n",
      "Epoch 59/500\n",
      "Epoch 00059: val_loss improved from 1.09562 to 1.09549, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0936 - acc: 0.5556 - val_loss: 1.0955 - val_acc: 0.3333\n",
      "Epoch 60/500\n",
      "Epoch 00060: val_loss improved from 1.09549 to 1.09536, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0935 - acc: 0.5556 - val_loss: 1.0954 - val_acc: 0.5000\n",
      "Epoch 61/500\n",
      "Epoch 00061: val_loss improved from 1.09536 to 1.09523, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0933 - acc: 0.5556 - val_loss: 1.0952 - val_acc: 0.5000\n",
      "Epoch 62/500\n",
      "Epoch 00062: val_loss improved from 1.09523 to 1.09510, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0931 - acc: 0.5556 - val_loss: 1.0951 - val_acc: 0.5000\n",
      "Epoch 63/500\n",
      "Epoch 00063: val_loss improved from 1.09510 to 1.09496, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0930 - acc: 0.5556 - val_loss: 1.0950 - val_acc: 0.5000\n",
      "Epoch 64/500\n",
      "Epoch 00064: val_loss improved from 1.09496 to 1.09482, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0928 - acc: 0.5556 - val_loss: 1.0948 - val_acc: 0.5000\n",
      "Epoch 65/500\n",
      "Epoch 00065: val_loss improved from 1.09482 to 1.09468, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0926 - acc: 0.5556 - val_loss: 1.0947 - val_acc: 0.5000\n",
      "Epoch 66/500\n",
      "Epoch 00066: val_loss improved from 1.09468 to 1.09454, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0924 - acc: 0.5556 - val_loss: 1.0945 - val_acc: 0.5000\n",
      "Epoch 67/500\n",
      "Epoch 00067: val_loss improved from 1.09454 to 1.09439, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0922 - acc: 0.5556 - val_loss: 1.0944 - val_acc: 0.5000\n",
      "Epoch 68/500\n",
      "Epoch 00068: val_loss improved from 1.09439 to 1.09424, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0920 - acc: 0.5556 - val_loss: 1.0942 - val_acc: 0.5000\n",
      "Epoch 69/500\n",
      "Epoch 00069: val_loss improved from 1.09424 to 1.09409, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0918 - acc: 0.5556 - val_loss: 1.0941 - val_acc: 0.5000\n",
      "Epoch 70/500\n",
      "Epoch 00070: val_loss improved from 1.09409 to 1.09393, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0916 - acc: 0.5556 - val_loss: 1.0939 - val_acc: 0.5000\n",
      "Epoch 71/500\n",
      "Epoch 00071: val_loss improved from 1.09393 to 1.09377, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0914 - acc: 0.5556 - val_loss: 1.0938 - val_acc: 0.5000\n",
      "Epoch 72/500\n",
      "Epoch 00072: val_loss improved from 1.09377 to 1.09361, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0912 - acc: 0.5556 - val_loss: 1.0936 - val_acc: 0.5000\n",
      "Epoch 73/500\n",
      "Epoch 00073: val_loss improved from 1.09361 to 1.09344, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0910 - acc: 0.5556 - val_loss: 1.0934 - val_acc: 0.5000\n",
      "Epoch 74/500\n",
      "Epoch 00074: val_loss improved from 1.09344 to 1.09328, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0908 - acc: 0.5556 - val_loss: 1.0933 - val_acc: 0.5000\n",
      "Epoch 75/500\n",
      "Epoch 00075: val_loss improved from 1.09328 to 1.09311, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0906 - acc: 0.5556 - val_loss: 1.0931 - val_acc: 0.5000\n",
      "Epoch 76/500\n",
      "Epoch 00076: val_loss improved from 1.09311 to 1.09293, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0903 - acc: 0.5556 - val_loss: 1.0929 - val_acc: 0.5000\n",
      "Epoch 77/500\n",
      "Epoch 00077: val_loss improved from 1.09293 to 1.09275, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0901 - acc: 0.5556 - val_loss: 1.0928 - val_acc: 0.5000\n",
      "Epoch 78/500\n",
      "Epoch 00078: val_loss improved from 1.09275 to 1.09257, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0899 - acc: 0.5556 - val_loss: 1.0926 - val_acc: 0.5000\n",
      "Epoch 79/500\n",
      "Epoch 00079: val_loss improved from 1.09257 to 1.09239, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0896 - acc: 0.5556 - val_loss: 1.0924 - val_acc: 0.5000\n",
      "Epoch 80/500\n",
      "Epoch 00080: val_loss improved from 1.09239 to 1.09220, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0894 - acc: 0.5556 - val_loss: 1.0922 - val_acc: 0.5000\n",
      "Epoch 81/500\n",
      "Epoch 00081: val_loss improved from 1.09220 to 1.09201, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0891 - acc: 0.5556 - val_loss: 1.0920 - val_acc: 0.5000\n",
      "Epoch 82/500\n",
      "Epoch 00082: val_loss improved from 1.09201 to 1.09182, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0889 - acc: 0.5556 - val_loss: 1.0918 - val_acc: 0.5000\n",
      "Epoch 83/500\n",
      "Epoch 00083: val_loss improved from 1.09182 to 1.09162, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0886 - acc: 0.5556 - val_loss: 1.0916 - val_acc: 0.5000\n",
      "Epoch 84/500\n",
      "Epoch 00084: val_loss improved from 1.09162 to 1.09142, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0884 - acc: 0.5556 - val_loss: 1.0914 - val_acc: 0.5000\n",
      "Epoch 85/500\n",
      "Epoch 00085: val_loss improved from 1.09142 to 1.09121, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0881 - acc: 0.5556 - val_loss: 1.0912 - val_acc: 0.5000\n",
      "Epoch 86/500\n",
      "Epoch 00086: val_loss improved from 1.09121 to 1.09100, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0879 - acc: 0.5556 - val_loss: 1.0910 - val_acc: 0.5000\n",
      "Epoch 87/500\n",
      "Epoch 00087: val_loss improved from 1.09100 to 1.09079, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0876 - acc: 0.5556 - val_loss: 1.0908 - val_acc: 0.5000\n",
      "Epoch 88/500\n",
      "Epoch 00088: val_loss improved from 1.09079 to 1.09058, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0873 - acc: 0.5556 - val_loss: 1.0906 - val_acc: 0.5000\n",
      "Epoch 89/500\n",
      "Epoch 00089: val_loss improved from 1.09058 to 1.09036, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0870 - acc: 0.5556 - val_loss: 1.0904 - val_acc: 0.5000\n",
      "Epoch 90/500\n",
      "Epoch 00090: val_loss improved from 1.09036 to 1.09014, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0867 - acc: 0.5556 - val_loss: 1.0901 - val_acc: 0.5000\n",
      "Epoch 91/500\n",
      "Epoch 00091: val_loss improved from 1.09014 to 1.08991, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0864 - acc: 0.5556 - val_loss: 1.0899 - val_acc: 0.5000\n",
      "Epoch 92/500\n",
      "Epoch 00092: val_loss improved from 1.08991 to 1.08968, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0862 - acc: 0.5556 - val_loss: 1.0897 - val_acc: 0.5000\n",
      "Epoch 93/500\n",
      "Epoch 00093: val_loss improved from 1.08968 to 1.08945, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0859 - acc: 0.5556 - val_loss: 1.0895 - val_acc: 0.5000\n",
      "Epoch 94/500\n",
      "Epoch 00094: val_loss improved from 1.08945 to 1.08921, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0855 - acc: 0.5556 - val_loss: 1.0892 - val_acc: 0.5000\n",
      "Epoch 95/500\n",
      "Epoch 00095: val_loss improved from 1.08921 to 1.08897, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0852 - acc: 0.5556 - val_loss: 1.0890 - val_acc: 0.5000\n",
      "Epoch 96/500\n",
      "Epoch 00096: val_loss improved from 1.08897 to 1.08873, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0849 - acc: 0.5556 - val_loss: 1.0887 - val_acc: 0.5000\n",
      "Epoch 97/500\n",
      "Epoch 00097: val_loss improved from 1.08873 to 1.08848, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0846 - acc: 0.5556 - val_loss: 1.0885 - val_acc: 0.5000\n",
      "Epoch 98/500\n",
      "Epoch 00098: val_loss improved from 1.08848 to 1.08823, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0843 - acc: 0.5556 - val_loss: 1.0882 - val_acc: 0.5000\n",
      "Epoch 99/500\n",
      "Epoch 00099: val_loss improved from 1.08823 to 1.08797, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0839 - acc: 0.5556 - val_loss: 1.0880 - val_acc: 0.5000\n",
      "Epoch 100/500\n",
      "Epoch 00100: val_loss improved from 1.08797 to 1.08771, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0836 - acc: 0.5556 - val_loss: 1.0877 - val_acc: 0.5000\n",
      "Epoch 101/500\n",
      "Epoch 00101: val_loss improved from 1.08771 to 1.08745, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0833 - acc: 0.5556 - val_loss: 1.0874 - val_acc: 0.5000\n",
      "Epoch 102/500\n",
      "Epoch 00102: val_loss improved from 1.08745 to 1.08718, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0829 - acc: 0.5556 - val_loss: 1.0872 - val_acc: 0.5000\n",
      "Epoch 103/500\n",
      "Epoch 00103: val_loss improved from 1.08718 to 1.08691, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0826 - acc: 0.6667 - val_loss: 1.0869 - val_acc: 0.5000\n",
      "Epoch 104/500\n",
      "Epoch 00104: val_loss improved from 1.08691 to 1.08663, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0822 - acc: 0.6667 - val_loss: 1.0866 - val_acc: 0.5000\n",
      "Epoch 105/500\n",
      "Epoch 00105: val_loss improved from 1.08663 to 1.08635, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0819 - acc: 0.6667 - val_loss: 1.0863 - val_acc: 0.5000\n",
      "Epoch 106/500\n",
      "Epoch 00106: val_loss improved from 1.08635 to 1.08606, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0815 - acc: 0.6667 - val_loss: 1.0861 - val_acc: 0.5000\n",
      "Epoch 107/500\n",
      "Epoch 00107: val_loss improved from 1.08606 to 1.08577, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0811 - acc: 0.6667 - val_loss: 1.0858 - val_acc: 0.5000\n",
      "Epoch 108/500\n",
      "Epoch 00108: val_loss improved from 1.08577 to 1.08547, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0807 - acc: 0.6667 - val_loss: 1.0855 - val_acc: 0.5000\n",
      "Epoch 109/500\n",
      "Epoch 00109: val_loss improved from 1.08547 to 1.08517, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0803 - acc: 0.6667 - val_loss: 1.0852 - val_acc: 0.5000\n",
      "Epoch 110/500\n",
      "Epoch 00110: val_loss improved from 1.08517 to 1.08486, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0799 - acc: 0.6667 - val_loss: 1.0849 - val_acc: 0.5000\n",
      "Epoch 111/500\n",
      "Epoch 00111: val_loss improved from 1.08486 to 1.08455, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0795 - acc: 0.6667 - val_loss: 1.0845 - val_acc: 0.5000\n",
      "Epoch 112/500\n",
      "Epoch 00112: val_loss improved from 1.08455 to 1.08423, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0791 - acc: 0.6667 - val_loss: 1.0842 - val_acc: 0.5000\n",
      "Epoch 113/500\n",
      "Epoch 00113: val_loss improved from 1.08423 to 1.08390, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0787 - acc: 0.6667 - val_loss: 1.0839 - val_acc: 0.5000\n",
      "Epoch 114/500\n",
      "Epoch 00114: val_loss improved from 1.08390 to 1.08357, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0783 - acc: 0.6667 - val_loss: 1.0836 - val_acc: 0.5000\n",
      "Epoch 115/500\n",
      "Epoch 00115: val_loss improved from 1.08357 to 1.08324, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0779 - acc: 0.6667 - val_loss: 1.0832 - val_acc: 0.5000\n",
      "Epoch 116/500\n",
      "Epoch 00116: val_loss improved from 1.08324 to 1.08289, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0774 - acc: 0.6667 - val_loss: 1.0829 - val_acc: 0.5000\n",
      "Epoch 117/500\n",
      "Epoch 00117: val_loss improved from 1.08289 to 1.08255, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0770 - acc: 0.6667 - val_loss: 1.0825 - val_acc: 0.5000\n",
      "Epoch 118/500\n",
      "Epoch 00118: val_loss improved from 1.08255 to 1.08219, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0765 - acc: 0.6667 - val_loss: 1.0822 - val_acc: 0.5000\n",
      "Epoch 119/500\n",
      "Epoch 00119: val_loss improved from 1.08219 to 1.08183, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0761 - acc: 0.6667 - val_loss: 1.0818 - val_acc: 0.5000\n",
      "Epoch 120/500\n",
      "Epoch 00120: val_loss improved from 1.08183 to 1.08145, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0756 - acc: 0.6667 - val_loss: 1.0815 - val_acc: 0.5000\n",
      "Epoch 121/500\n",
      "Epoch 00121: val_loss improved from 1.08145 to 1.08108, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0751 - acc: 0.6667 - val_loss: 1.0811 - val_acc: 0.5000\n",
      "Epoch 122/500\n",
      "Epoch 00122: val_loss improved from 1.08108 to 1.08069, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0746 - acc: 0.6667 - val_loss: 1.0807 - val_acc: 0.5000\n",
      "Epoch 123/500\n",
      "Epoch 00123: val_loss improved from 1.08069 to 1.08030, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0741 - acc: 0.7778 - val_loss: 1.0803 - val_acc: 0.5000\n",
      "Epoch 124/500\n",
      "Epoch 00124: val_loss improved from 1.08030 to 1.07987, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0736 - acc: 0.6667 - val_loss: 1.0799 - val_acc: 0.5000\n",
      "Epoch 125/500\n",
      "Epoch 00125: val_loss improved from 1.07987 to 1.07952, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0731 - acc: 0.7778 - val_loss: 1.0795 - val_acc: 0.6667\n",
      "Epoch 126/500\n",
      "Epoch 00126: val_loss improved from 1.07952 to 1.07902, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0725 - acc: 0.6667 - val_loss: 1.0790 - val_acc: 0.5000\n",
      "Epoch 127/500\n",
      "Epoch 00127: val_loss improved from 1.07902 to 1.07877, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0720 - acc: 0.6667 - val_loss: 1.0788 - val_acc: 0.6667\n",
      "Epoch 128/500\n",
      "Epoch 00128: val_loss improved from 1.07877 to 1.07817, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0715 - acc: 0.7778 - val_loss: 1.0782 - val_acc: 0.5000\n",
      "Epoch 129/500\n",
      "Epoch 00129: val_loss improved from 1.07817 to 1.07789, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0709 - acc: 0.6667 - val_loss: 1.0779 - val_acc: 0.8333\n",
      "Epoch 130/500\n",
      "Epoch 00130: val_loss improved from 1.07789 to 1.07730, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0703 - acc: 0.7778 - val_loss: 1.0773 - val_acc: 0.5000\n",
      "Epoch 131/500\n",
      "Epoch 00131: val_loss improved from 1.07730 to 1.07696, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0697 - acc: 0.6667 - val_loss: 1.0770 - val_acc: 0.6667\n",
      "Epoch 132/500\n",
      "Epoch 00132: val_loss improved from 1.07696 to 1.07638, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0691 - acc: 0.6667 - val_loss: 1.0764 - val_acc: 0.5000\n",
      "Epoch 133/500\n",
      "Epoch 00133: val_loss improved from 1.07638 to 1.07600, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0685 - acc: 0.6667 - val_loss: 1.0760 - val_acc: 0.6667\n",
      "Epoch 134/500\n",
      "Epoch 00134: val_loss improved from 1.07600 to 1.07540, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0679 - acc: 0.6667 - val_loss: 1.0754 - val_acc: 0.5000\n",
      "Epoch 135/500\n",
      "Epoch 00135: val_loss improved from 1.07540 to 1.07502, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0672 - acc: 0.6667 - val_loss: 1.0750 - val_acc: 0.6667\n",
      "Epoch 136/500\n",
      "Epoch 00136: val_loss improved from 1.07502 to 1.07436, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0666 - acc: 0.6667 - val_loss: 1.0744 - val_acc: 0.5000\n",
      "Epoch 137/500\n",
      "Epoch 00137: val_loss improved from 1.07436 to 1.07400, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0659 - acc: 0.6667 - val_loss: 1.0740 - val_acc: 0.6667\n",
      "Epoch 138/500\n",
      "Epoch 00138: val_loss improved from 1.07400 to 1.07326, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0652 - acc: 0.6667 - val_loss: 1.0733 - val_acc: 0.5000\n",
      "Epoch 139/500\n",
      "Epoch 00139: val_loss improved from 1.07326 to 1.07296, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0645 - acc: 0.6667 - val_loss: 1.0730 - val_acc: 0.8333\n",
      "Epoch 140/500\n",
      "Epoch 00140: val_loss improved from 1.07296 to 1.07210, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0638 - acc: 0.7778 - val_loss: 1.0721 - val_acc: 0.5000\n",
      "Epoch 141/500\n",
      "Epoch 00141: val_loss improved from 1.07210 to 1.07183, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0630 - acc: 0.6667 - val_loss: 1.0718 - val_acc: 0.8333\n",
      "Epoch 142/500\n",
      "Epoch 00142: val_loss improved from 1.07183 to 1.07089, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0623 - acc: 0.7778 - val_loss: 1.0709 - val_acc: 0.5000\n",
      "Epoch 143/500\n",
      "Epoch 00143: val_loss improved from 1.07089 to 1.07060, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0615 - acc: 0.6667 - val_loss: 1.0706 - val_acc: 0.8333\n",
      "Epoch 144/500\n",
      "Epoch 00144: val_loss improved from 1.07060 to 1.06959, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0606 - acc: 0.6667 - val_loss: 1.0696 - val_acc: 0.5000\n",
      "Epoch 145/500\n",
      "Epoch 00145: val_loss improved from 1.06959 to 1.06929, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0598 - acc: 0.6667 - val_loss: 1.0693 - val_acc: 0.6667\n",
      "Epoch 146/500\n",
      "Epoch 00146: val_loss improved from 1.06929 to 1.06819, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0589 - acc: 0.6667 - val_loss: 1.0682 - val_acc: 0.5000\n",
      "Epoch 147/500\n",
      "Epoch 00147: val_loss improved from 1.06819 to 1.06792, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0580 - acc: 0.6667 - val_loss: 1.0679 - val_acc: 0.6667\n",
      "Epoch 148/500\n",
      "Epoch 00148: val_loss improved from 1.06792 to 1.06668, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0571 - acc: 0.6667 - val_loss: 1.0667 - val_acc: 0.5000\n",
      "Epoch 149/500\n",
      "Epoch 00149: val_loss improved from 1.06668 to 1.06649, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0561 - acc: 0.6667 - val_loss: 1.0665 - val_acc: 0.8333\n",
      "Epoch 150/500\n",
      "Epoch 00150: val_loss improved from 1.06649 to 1.06505, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0551 - acc: 0.6667 - val_loss: 1.0650 - val_acc: 0.5000\n",
      "Epoch 151/500\n",
      "Epoch 00151: val_loss improved from 1.06505 to 1.06495, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0541 - acc: 0.6667 - val_loss: 1.0650 - val_acc: 0.8333\n",
      "Epoch 152/500\n",
      "Epoch 00152: val_loss improved from 1.06495 to 1.06329, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0530 - acc: 0.6667 - val_loss: 1.0633 - val_acc: 0.5000\n",
      "Epoch 153/500\n",
      "Epoch 00153: val_loss improved from 1.06329 to 1.06328, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0519 - acc: 0.6667 - val_loss: 1.0633 - val_acc: 0.8333\n",
      "Epoch 154/500\n",
      "Epoch 00154: val_loss improved from 1.06328 to 1.06137, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0508 - acc: 0.6667 - val_loss: 1.0614 - val_acc: 0.5000\n",
      "Epoch 155/500\n",
      "Epoch 00155: val_loss did not improve\n",
      " - 3s - loss: 1.0495 - acc: 0.6667 - val_loss: 1.0615 - val_acc: 0.6667\n",
      "Epoch 156/500\n",
      "Epoch 00156: val_loss improved from 1.06137 to 1.05927, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0483 - acc: 0.6667 - val_loss: 1.0593 - val_acc: 0.5000\n",
      "Epoch 157/500\n",
      "Epoch 00157: val_loss did not improve\n",
      " - 3s - loss: 1.0470 - acc: 0.6667 - val_loss: 1.0595 - val_acc: 0.6667\n",
      "Epoch 158/500\n",
      "Epoch 00158: val_loss improved from 1.05927 to 1.05695, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0457 - acc: 0.6667 - val_loss: 1.0570 - val_acc: 0.5000\n",
      "Epoch 159/500\n",
      "Epoch 00159: val_loss did not improve\n",
      " - 3s - loss: 1.0443 - acc: 0.6667 - val_loss: 1.0575 - val_acc: 0.6667\n",
      "Epoch 160/500\n",
      "Epoch 00160: val_loss improved from 1.05695 to 1.05440, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0429 - acc: 0.6667 - val_loss: 1.0544 - val_acc: 0.5000\n",
      "Epoch 161/500\n",
      "Epoch 00161: val_loss did not improve\n",
      " - 3s - loss: 1.0414 - acc: 0.6667 - val_loss: 1.0553 - val_acc: 0.6667\n",
      "Epoch 162/500\n",
      "Epoch 00162: val_loss improved from 1.05440 to 1.05159, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0399 - acc: 0.6667 - val_loss: 1.0516 - val_acc: 0.5000\n",
      "Epoch 163/500\n",
      "Epoch 00163: val_loss did not improve\n",
      " - 3s - loss: 1.0383 - acc: 0.6667 - val_loss: 1.0529 - val_acc: 0.6667\n",
      "Epoch 164/500\n",
      "Epoch 00164: val_loss improved from 1.05159 to 1.04847, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0366 - acc: 0.6667 - val_loss: 1.0485 - val_acc: 0.5000\n",
      "Epoch 165/500\n",
      "Epoch 00165: val_loss did not improve\n",
      " - 3s - loss: 1.0349 - acc: 0.6667 - val_loss: 1.0504 - val_acc: 0.6667\n",
      "Epoch 166/500\n",
      "Epoch 00166: val_loss improved from 1.04847 to 1.04501, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0331 - acc: 0.6667 - val_loss: 1.0450 - val_acc: 0.5000\n",
      "Epoch 167/500\n",
      "Epoch 00167: val_loss did not improve\n",
      " - 3s - loss: 1.0313 - acc: 0.6667 - val_loss: 1.0477 - val_acc: 0.6667\n",
      "Epoch 168/500\n",
      "Epoch 00168: val_loss improved from 1.04501 to 1.04118, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0294 - acc: 0.6667 - val_loss: 1.0412 - val_acc: 0.5000\n",
      "Epoch 169/500\n",
      "Epoch 00169: val_loss did not improve\n",
      " - 3s - loss: 1.0274 - acc: 0.6667 - val_loss: 1.0449 - val_acc: 0.6667\n",
      "Epoch 170/500\n",
      "Epoch 00170: val_loss improved from 1.04118 to 1.03695, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0254 - acc: 0.6667 - val_loss: 1.0370 - val_acc: 0.5000\n",
      "Epoch 171/500\n",
      "Epoch 00171: val_loss did not improve\n",
      " - 3s - loss: 1.0233 - acc: 0.5556 - val_loss: 1.0420 - val_acc: 0.6667\n",
      "Epoch 172/500\n",
      "Epoch 00172: val_loss improved from 1.03695 to 1.03235, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0211 - acc: 0.6667 - val_loss: 1.0323 - val_acc: 0.5000\n",
      "Epoch 173/500\n",
      "Epoch 00173: val_loss did not improve\n",
      " - 3s - loss: 1.0190 - acc: 0.5556 - val_loss: 1.0389 - val_acc: 0.6667\n",
      "Epoch 174/500\n",
      "Epoch 00174: val_loss improved from 1.03235 to 1.02739, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0166 - acc: 0.6667 - val_loss: 1.0274 - val_acc: 0.5000\n",
      "Epoch 175/500\n",
      "Epoch 00175: val_loss did not improve\n",
      " - 3s - loss: 1.0145 - acc: 0.5556 - val_loss: 1.0357 - val_acc: 0.6667\n",
      "Epoch 176/500\n",
      "Epoch 00176: val_loss improved from 1.02739 to 1.02216, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0119 - acc: 0.6667 - val_loss: 1.0222 - val_acc: 0.5000\n",
      "Epoch 177/500\n",
      "Epoch 00177: val_loss did not improve\n",
      " - 3s - loss: 1.0099 - acc: 0.5556 - val_loss: 1.0326 - val_acc: 0.6667\n",
      "Epoch 178/500\n",
      "Epoch 00178: val_loss improved from 1.02216 to 1.01675, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0071 - acc: 0.6667 - val_loss: 1.0168 - val_acc: 0.5000\n",
      "Epoch 179/500\n",
      "Epoch 00179: val_loss did not improve\n",
      " - 3s - loss: 1.0053 - acc: 0.5556 - val_loss: 1.0295 - val_acc: 0.6667\n",
      "Epoch 180/500\n",
      "Epoch 00180: val_loss improved from 1.01675 to 1.01129, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 1.0023 - acc: 0.6667 - val_loss: 1.0113 - val_acc: 0.5000\n",
      "Epoch 181/500\n",
      "Epoch 00181: val_loss did not improve\n",
      " - 3s - loss: 1.0009 - acc: 0.5556 - val_loss: 1.0265 - val_acc: 0.6667\n",
      "Epoch 182/500\n",
      "Epoch 00182: val_loss improved from 1.01129 to 1.00593, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9975 - acc: 0.6667 - val_loss: 1.0059 - val_acc: 0.5000\n",
      "Epoch 183/500\n",
      "Epoch 00183: val_loss did not improve\n",
      " - 3s - loss: 0.9966 - acc: 0.5556 - val_loss: 1.0232 - val_acc: 0.5000\n",
      "Epoch 184/500\n",
      "Epoch 00184: val_loss improved from 1.00593 to 1.00087, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9927 - acc: 0.6667 - val_loss: 1.0009 - val_acc: 0.5000\n",
      "Epoch 185/500\n",
      "Epoch 00185: val_loss did not improve\n",
      " - 3s - loss: 0.9925 - acc: 0.5556 - val_loss: 1.0195 - val_acc: 0.5000\n",
      "Epoch 186/500\n",
      "Epoch 00186: val_loss improved from 1.00087 to 0.99610, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9880 - acc: 0.6667 - val_loss: 0.9961 - val_acc: 0.5000\n",
      "Epoch 187/500\n",
      "Epoch 00187: val_loss did not improve\n",
      " - 3s - loss: 0.9886 - acc: 0.5556 - val_loss: 1.0154 - val_acc: 0.5000\n",
      "Epoch 188/500\n",
      "Epoch 00188: val_loss improved from 0.99610 to 0.99160, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9833 - acc: 0.6667 - val_loss: 0.9916 - val_acc: 0.5000\n",
      "Epoch 189/500\n",
      "Epoch 00189: val_loss did not improve\n",
      " - 3s - loss: 0.9850 - acc: 0.5556 - val_loss: 1.0108 - val_acc: 0.5000\n",
      "Epoch 190/500\n",
      "Epoch 00190: val_loss improved from 0.99160 to 0.98710, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9787 - acc: 0.6667 - val_loss: 0.9871 - val_acc: 0.5000\n",
      "Epoch 191/500\n",
      "Epoch 00191: val_loss did not improve\n",
      " - 3s - loss: 0.9817 - acc: 0.5556 - val_loss: 1.0057 - val_acc: 0.5000\n",
      "Epoch 192/500\n",
      "Epoch 00192: val_loss improved from 0.98710 to 0.98289, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9740 - acc: 0.6667 - val_loss: 0.9829 - val_acc: 0.5000\n",
      "Epoch 193/500\n",
      "Epoch 00193: val_loss did not improve\n",
      " - 3s - loss: 0.9784 - acc: 0.5556 - val_loss: 1.0012 - val_acc: 0.5000\n",
      "Epoch 194/500\n",
      "Epoch 00194: val_loss improved from 0.98289 to 0.97777, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9694 - acc: 0.6667 - val_loss: 0.9778 - val_acc: 0.5000\n",
      "Epoch 195/500\n",
      "Epoch 00195: val_loss did not improve\n",
      " - 3s - loss: 0.9753 - acc: 0.5556 - val_loss: 0.9962 - val_acc: 0.5000\n",
      "Epoch 196/500\n",
      "Epoch 00196: val_loss did not improve\n",
      " - 3s - loss: 0.9644 - acc: 0.6667 - val_loss: 0.9835 - val_acc: 0.5000\n",
      "Epoch 197/500\n",
      "Epoch 00197: val_loss did not improve\n",
      " - 3s - loss: 0.9698 - acc: 0.5556 - val_loss: 1.0110 - val_acc: 0.6667\n",
      "Epoch 198/500\n",
      "Epoch 00198: val_loss improved from 0.97777 to 0.97767, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9698 - acc: 0.5556 - val_loss: 0.9777 - val_acc: 0.5000\n",
      "Epoch 199/500\n",
      "Epoch 00199: val_loss did not improve\n",
      " - 3s - loss: 0.9684 - acc: 0.5556 - val_loss: 0.9926 - val_acc: 0.5000\n",
      "Epoch 200/500\n",
      "Epoch 00200: val_loss improved from 0.97767 to 0.94062, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9588 - acc: 0.6667 - val_loss: 0.9406 - val_acc: 0.6667\n",
      "Epoch 201/500\n",
      "Epoch 00201: val_loss did not improve\n",
      " - 3s - loss: 0.9691 - acc: 0.5556 - val_loss: 0.9735 - val_acc: 0.5000\n",
      "Epoch 202/500\n",
      "Epoch 00202: val_loss did not improve\n",
      " - 3s - loss: 0.9641 - acc: 0.5556 - val_loss: 0.9863 - val_acc: 0.5000\n",
      "Epoch 203/500\n",
      "Epoch 00203: val_loss improved from 0.94062 to 0.93678, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.9502 - acc: 0.6667 - val_loss: 0.9368 - val_acc: 0.5000\n",
      "Epoch 204/500\n",
      "Epoch 00204: val_loss did not improve\n",
      " - 3s - loss: 0.9633 - acc: 0.5556 - val_loss: 0.9726 - val_acc: 0.5000\n",
      "Epoch 205/500\n",
      "Epoch 00205: val_loss did not improve\n",
      " - 3s - loss: 0.9590 - acc: 0.5556 - val_loss: 0.9836 - val_acc: 0.5000\n",
      "Epoch 206/500\n",
      "Epoch 00206: val_loss did not improve\n",
      " - 3s - loss: 0.9460 - acc: 0.6667 - val_loss: 0.9564 - val_acc: 0.6667\n",
      "Epoch 207/500\n",
      "Epoch 00207: val_loss did not improve\n",
      " - 3s - loss: 0.9746 - acc: 0.5556 - val_loss: 1.0028 - val_acc: 0.6667\n",
      "Epoch 208/500\n",
      "Epoch 00208: val_loss did not improve\n",
      " - 3s - loss: 0.9603 - acc: 0.5556 - val_loss: 0.9906 - val_acc: 0.5000\n",
      "Epoch 209/500\n",
      "Epoch 00209: val_loss did not improve\n",
      " - 3s - loss: 0.9562 - acc: 0.5556 - val_loss: 0.9845 - val_acc: 0.5000\n",
      "Epoch 210/500\n",
      "Epoch 00210: val_loss did not improve\n",
      " - 3s - loss: 0.9517 - acc: 0.5556 - val_loss: 0.9799 - val_acc: 0.5000\n",
      "Epoch 211/500\n",
      "Epoch 00211: val_loss did not improve\n",
      " - 3s - loss: 0.9450 - acc: 0.6667 - val_loss: 0.9749 - val_acc: 0.5000\n",
      "Epoch 212/500\n",
      "Epoch 00212: val_loss did not improve\n",
      " - 3s - loss: 0.9343 - acc: 0.5556 - val_loss: 0.9736 - val_acc: 0.5000\n",
      "Epoch 213/500\n",
      "Epoch 00213: val_loss did not improve\n",
      " - 3s - loss: 0.9315 - acc: 0.5556 - val_loss: 0.9704 - val_acc: 0.5000\n",
      "Epoch 214/500\n",
      "Epoch 00214: val_loss did not improve\n",
      " - 3s - loss: 0.9384 - acc: 0.5556 - val_loss: 0.9918 - val_acc: 0.6667\n",
      "Epoch 215/500\n",
      "Epoch 00215: val_loss did not improve\n",
      " - 3s - loss: 0.9514 - acc: 0.5556 - val_loss: 0.9793 - val_acc: 0.5000\n",
      "Epoch 216/500\n",
      "Epoch 00216: val_loss did not improve\n",
      " - 3s - loss: 0.9473 - acc: 0.5556 - val_loss: 0.9740 - val_acc: 0.5000\n",
      "Epoch 217/500\n",
      "Epoch 00217: val_loss did not improve\n",
      " - 3s - loss: 0.9421 - acc: 0.5556 - val_loss: 0.9694 - val_acc: 0.5000\n",
      "Epoch 218/500\n",
      "Epoch 00218: val_loss did not improve\n",
      " - 3s - loss: 0.9308 - acc: 0.5556 - val_loss: 0.9616 - val_acc: 0.5000\n",
      "Epoch 219/500\n",
      "Epoch 00219: val_loss did not improve\n",
      " - 3s - loss: 0.9446 - acc: 0.5556 - val_loss: 0.9651 - val_acc: 0.5000\n",
      "Epoch 220/500\n",
      "Epoch 00220: val_loss did not improve\n",
      " - 3s - loss: 0.9207 - acc: 0.6667 - val_loss: 1.0132 - val_acc: 0.3333\n",
      "Epoch 221/500\n",
      "Epoch 00221: val_loss did not improve\n",
      " - 3s - loss: 0.9466 - acc: 0.5556 - val_loss: 0.9746 - val_acc: 0.5000\n",
      "Epoch 222/500\n",
      "Epoch 00222: val_loss did not improve\n",
      " - 3s - loss: 0.9421 - acc: 0.5556 - val_loss: 0.9677 - val_acc: 0.5000\n",
      "Epoch 223/500\n",
      "Epoch 00223: val_loss did not improve\n",
      " - 3s - loss: 0.9375 - acc: 0.5556 - val_loss: 0.9634 - val_acc: 0.5000\n",
      "Epoch 224/500\n",
      "Epoch 00224: val_loss did not improve\n",
      " - 3s - loss: 0.9281 - acc: 0.6667 - val_loss: 0.9564 - val_acc: 0.5000\n",
      "Epoch 225/500\n",
      "Epoch 00225: val_loss did not improve\n",
      " - 3s - loss: 0.9376 - acc: 0.5556 - val_loss: 0.9618 - val_acc: 0.5000\n",
      "Epoch 226/500\n",
      "Epoch 00226: val_loss did not improve\n",
      " - 3s - loss: 0.9260 - acc: 0.6667 - val_loss: 0.9539 - val_acc: 0.5000\n",
      "Epoch 227/500\n",
      "Epoch 00227: val_loss did not improve\n",
      " - 3s - loss: 0.9361 - acc: 0.5556 - val_loss: 0.9592 - val_acc: 0.5000\n",
      "Epoch 228/500\n",
      "Epoch 00228: val_loss did not improve\n",
      " - 3s - loss: 0.9195 - acc: 0.5556 - val_loss: 0.9469 - val_acc: 0.5000\n",
      "Epoch 229/500\n",
      "Epoch 00229: val_loss did not improve\n",
      " - 3s - loss: 0.9377 - acc: 0.5556 - val_loss: 0.9514 - val_acc: 0.5000\n",
      "Epoch 230/500\n",
      "Epoch 00230: val_loss did not improve\n",
      " - 3s - loss: 0.9319 - acc: 0.5556 - val_loss: 0.9578 - val_acc: 0.5000\n",
      "Epoch 231/500\n",
      "Epoch 00231: val_loss did not improve\n",
      " - 3s - loss: 0.9202 - acc: 0.6667 - val_loss: 0.9461 - val_acc: 0.5000\n",
      "Epoch 232/500\n",
      "Epoch 00232: val_loss did not improve\n",
      " - 3s - loss: 0.9332 - acc: 0.5556 - val_loss: 0.9507 - val_acc: 0.5000\n",
      "Epoch 233/500\n",
      "Epoch 00233: val_loss did not improve\n",
      " - 3s - loss: 0.9170 - acc: 0.5556 - val_loss: 1.0967 - val_acc: 0.3333\n",
      "Epoch 234/500\n",
      "Epoch 00234: val_loss did not improve\n",
      " - 3s - loss: 0.9661 - acc: 0.5556 - val_loss: 0.9559 - val_acc: 0.5000\n",
      "Epoch 235/500\n",
      "Epoch 00235: val_loss did not improve\n",
      " - 3s - loss: 0.9230 - acc: 0.5556 - val_loss: 0.9504 - val_acc: 0.5000\n",
      "Epoch 236/500\n",
      "Epoch 00236: val_loss improved from 0.93678 to 0.87293, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.8999 - acc: 0.5556 - val_loss: 0.8729 - val_acc: 0.6667\n",
      "Epoch 237/500\n",
      "Epoch 00237: val_loss did not improve\n",
      " - 3s - loss: 0.9345 - acc: 0.5556 - val_loss: 0.8730 - val_acc: 0.6667\n",
      "Epoch 238/500\n",
      "Epoch 00238: val_loss did not improve\n",
      " - 3s - loss: 0.9320 - acc: 0.5556 - val_loss: 0.8804 - val_acc: 0.5000\n",
      "Epoch 239/500\n",
      "Epoch 00239: val_loss did not improve\n",
      " - 3s - loss: 0.9298 - acc: 0.5556 - val_loss: 0.9050 - val_acc: 0.5000\n",
      "Epoch 240/500\n",
      "Epoch 00240: val_loss did not improve\n",
      " - 3s - loss: 0.9276 - acc: 0.5556 - val_loss: 0.9263 - val_acc: 0.5000\n",
      "Epoch 241/500\n",
      "Epoch 00241: val_loss did not improve\n",
      " - 3s - loss: 0.9255 - acc: 0.5556 - val_loss: 0.9352 - val_acc: 0.5000\n",
      "Epoch 242/500\n",
      "Epoch 00242: val_loss did not improve\n",
      " - 3s - loss: 0.9233 - acc: 0.5556 - val_loss: 0.9392 - val_acc: 0.5000\n",
      "Epoch 243/500\n",
      "Epoch 00243: val_loss did not improve\n",
      " - 3s - loss: 0.9206 - acc: 0.5556 - val_loss: 0.9421 - val_acc: 0.5000\n",
      "Epoch 244/500\n",
      "Epoch 00244: val_loss did not improve\n",
      " - 3s - loss: 0.9115 - acc: 0.5556 - val_loss: 1.0033 - val_acc: 0.3333\n",
      "Epoch 245/500\n",
      "Epoch 00245: val_loss did not improve\n",
      " - 3s - loss: 0.9291 - acc: 0.5556 - val_loss: 0.9514 - val_acc: 0.5000\n",
      "Epoch 246/500\n",
      "Epoch 00246: val_loss did not improve\n",
      " - 3s - loss: 0.9222 - acc: 0.5556 - val_loss: 0.9446 - val_acc: 0.5000\n",
      "Epoch 247/500\n",
      "Epoch 00247: val_loss did not improve\n",
      " - 3s - loss: 0.9113 - acc: 0.5556 - val_loss: 0.9353 - val_acc: 0.5000\n",
      "Epoch 248/500\n",
      "Epoch 00248: val_loss did not improve\n",
      " - 3s - loss: 0.9135 - acc: 0.5556 - val_loss: 0.9417 - val_acc: 0.5000\n",
      "Epoch 249/500\n",
      "Epoch 00249: val_loss did not improve\n",
      " - 3s - loss: 0.9015 - acc: 0.6667 - val_loss: 0.9257 - val_acc: 0.5000\n",
      "Epoch 250/500\n",
      "Epoch 00250: val_loss did not improve\n",
      " - 3s - loss: 0.9168 - acc: 0.5556 - val_loss: 0.9308 - val_acc: 0.5000\n",
      "Epoch 251/500\n",
      "Epoch 00251: val_loss did not improve\n",
      " - 3s - loss: 0.9127 - acc: 0.5556 - val_loss: 0.9351 - val_acc: 0.5000\n",
      "Epoch 252/500\n",
      "Epoch 00252: val_loss did not improve\n",
      " - 3s - loss: 0.8579 - acc: 0.6667 - val_loss: 1.0649 - val_acc: 0.5000\n",
      "Epoch 253/500\n",
      "Epoch 00253: val_loss did not improve\n",
      " - 3s - loss: 0.9213 - acc: 0.5556 - val_loss: 0.9421 - val_acc: 0.5000\n",
      "Epoch 254/500\n",
      "Epoch 00254: val_loss did not improve\n",
      " - 3s - loss: 0.9137 - acc: 0.5556 - val_loss: 0.9350 - val_acc: 0.5000\n",
      "Epoch 255/500\n",
      "Epoch 00255: val_loss did not improve\n",
      " - 3s - loss: 0.9051 - acc: 0.5556 - val_loss: 0.9283 - val_acc: 0.5000\n",
      "Epoch 256/500\n",
      "Epoch 00256: val_loss did not improve\n",
      " - 3s - loss: 0.8570 - acc: 0.6667 - val_loss: 1.1308 - val_acc: 0.3333\n",
      "Epoch 257/500\n",
      "Epoch 00257: val_loss did not improve\n",
      " - 3s - loss: 1.0933 - acc: 0.3333 - val_loss: 1.1055 - val_acc: 0.3333\n",
      "Epoch 258/500\n",
      "Epoch 00258: val_loss did not improve\n",
      " - 3s - loss: 0.9415 - acc: 0.5556 - val_loss: 0.9695 - val_acc: 0.5000\n",
      "Epoch 259/500\n",
      "Epoch 00259: val_loss did not improve\n",
      " - 3s - loss: 0.9101 - acc: 0.5556 - val_loss: 0.9434 - val_acc: 0.6667\n",
      "Epoch 260/500\n",
      "Epoch 00260: val_loss did not improve\n",
      " - 3s - loss: 0.9071 - acc: 0.5556 - val_loss: 0.9332 - val_acc: 0.5000\n",
      "Epoch 261/500\n",
      "Epoch 00261: val_loss did not improve\n",
      " - 3s - loss: 0.9039 - acc: 0.5556 - val_loss: 0.9291 - val_acc: 0.5000\n",
      "Epoch 262/500\n",
      "Epoch 00262: val_loss did not improve\n",
      " - 3s - loss: 0.8994 - acc: 0.5556 - val_loss: 0.9258 - val_acc: 0.5000\n",
      "Epoch 263/500\n",
      "Epoch 00263: val_loss did not improve\n",
      " - 3s - loss: 0.8873 - acc: 0.6667 - val_loss: 0.9207 - val_acc: 0.5000\n",
      "Epoch 264/500\n",
      "Epoch 00264: val_loss did not improve\n",
      " - 3s - loss: 0.8432 - acc: 0.6667 - val_loss: 1.0969 - val_acc: 0.3333\n",
      "Epoch 265/500\n",
      "Epoch 00265: val_loss did not improve\n",
      " - 3s - loss: 0.9204 - acc: 0.5556 - val_loss: 0.9232 - val_acc: 0.5000\n",
      "Epoch 266/500\n",
      "Epoch 00266: val_loss did not improve\n",
      " - 3s - loss: 0.8914 - acc: 0.5556 - val_loss: 0.9200 - val_acc: 0.5000\n",
      "Epoch 267/500\n",
      "Epoch 00267: val_loss did not improve\n",
      " - 3s - loss: 0.8722 - acc: 0.5556 - val_loss: 0.9169 - val_acc: 0.5000\n",
      "Epoch 268/500\n",
      "Epoch 00268: val_loss did not improve\n",
      " - 3s - loss: 0.8277 - acc: 0.6667 - val_loss: 0.9091 - val_acc: 0.5000\n",
      "Epoch 269/500\n",
      "Epoch 00269: val_loss did not improve\n",
      " - 3s - loss: 0.9075 - acc: 0.5556 - val_loss: 0.9108 - val_acc: 0.5000\n",
      "Epoch 270/500\n",
      "Epoch 00270: val_loss did not improve\n",
      " - 3s - loss: 0.9046 - acc: 0.5556 - val_loss: 0.9121 - val_acc: 0.5000\n",
      "Epoch 271/500\n",
      "Epoch 00271: val_loss did not improve\n",
      " - 3s - loss: 0.8999 - acc: 0.5556 - val_loss: 0.9140 - val_acc: 0.5000\n",
      "Epoch 272/500\n",
      "Epoch 00272: val_loss did not improve\n",
      " - 3s - loss: 0.8193 - acc: 0.6667 - val_loss: 0.9394 - val_acc: 0.6667\n",
      "Epoch 273/500\n",
      "Epoch 00273: val_loss did not improve\n",
      " - 3s - loss: 0.8963 - acc: 0.5556 - val_loss: 0.9193 - val_acc: 0.5000\n",
      "Epoch 274/500\n",
      "Epoch 00274: val_loss did not improve\n",
      " - 3s - loss: 0.8930 - acc: 0.5556 - val_loss: 0.9152 - val_acc: 0.5000\n",
      "Epoch 275/500\n",
      "Epoch 00275: val_loss did not improve\n",
      " - 3s - loss: 0.8883 - acc: 0.5556 - val_loss: 0.9120 - val_acc: 0.5000\n",
      "Epoch 276/500\n",
      "Epoch 00276: val_loss did not improve\n",
      " - 3s - loss: 0.8697 - acc: 0.6667 - val_loss: 0.9058 - val_acc: 0.5000\n",
      "Epoch 277/500\n",
      "Epoch 00277: val_loss did not improve\n",
      " - 3s - loss: 0.8969 - acc: 0.5556 - val_loss: 0.9075 - val_acc: 0.5000\n",
      "Epoch 278/500\n",
      "Epoch 00278: val_loss did not improve\n",
      " - 3s - loss: 0.8082 - acc: 0.6667 - val_loss: 0.9163 - val_acc: 0.5000\n",
      "Epoch 279/500\n",
      "Epoch 00279: val_loss did not improve\n",
      " - 3s - loss: 0.8885 - acc: 0.5556 - val_loss: 0.9098 - val_acc: 0.5000\n",
      "Epoch 280/500\n",
      "Epoch 00280: val_loss did not improve\n",
      " - 3s - loss: 0.8837 - acc: 0.5556 - val_loss: 0.9063 - val_acc: 0.5000\n",
      "Epoch 281/500\n",
      "Epoch 00281: val_loss did not improve\n",
      " - 3s - loss: 0.8633 - acc: 0.6667 - val_loss: 0.8992 - val_acc: 0.5000\n",
      "Epoch 282/500\n",
      "Epoch 00282: val_loss did not improve\n",
      " - 3s - loss: 0.8958 - acc: 0.5556 - val_loss: 0.9003 - val_acc: 0.5000\n",
      "Epoch 283/500\n",
      "Epoch 00283: val_loss did not improve\n",
      " - 3s - loss: 0.8874 - acc: 0.5556 - val_loss: 0.9037 - val_acc: 0.5000\n",
      "Epoch 284/500\n",
      "Epoch 00284: val_loss did not improve\n",
      " - 3s - loss: 0.8554 - acc: 0.5556 - val_loss: 0.8984 - val_acc: 0.5000\n",
      "Epoch 285/500\n",
      "Epoch 00285: val_loss did not improve\n",
      " - 3s - loss: 0.8400 - acc: 0.5556 - val_loss: 1.1387 - val_acc: 0.3333\n",
      "Epoch 286/500\n",
      "Epoch 00286: val_loss did not improve\n",
      " - 3s - loss: 1.0926 - acc: 0.3333 - val_loss: 1.1150 - val_acc: 0.3333\n",
      "Epoch 287/500\n",
      "Epoch 00287: val_loss did not improve\n",
      " - 3s - loss: 0.9424 - acc: 0.5556 - val_loss: 0.9045 - val_acc: 0.5000\n",
      "Epoch 288/500\n",
      "Epoch 00288: val_loss did not improve\n",
      " - 3s - loss: 0.8730 - acc: 0.5556 - val_loss: 0.9012 - val_acc: 0.5000\n",
      "Epoch 289/500\n",
      "Epoch 00289: val_loss did not improve\n",
      " - 3s - loss: 0.8607 - acc: 0.6667 - val_loss: 0.8972 - val_acc: 0.5000\n",
      "Epoch 290/500\n",
      "Epoch 00290: val_loss did not improve\n",
      " - 3s - loss: 0.8417 - acc: 0.5556 - val_loss: 0.8943 - val_acc: 0.5000\n",
      "Epoch 291/500\n",
      "Epoch 00291: val_loss did not improve\n",
      " - 3s - loss: 0.7963 - acc: 0.6667 - val_loss: 0.8916 - val_acc: 0.5000\n",
      "Epoch 292/500\n",
      "Epoch 00292: val_loss did not improve\n",
      " - 3s - loss: 0.8646 - acc: 0.5556 - val_loss: 0.9332 - val_acc: 0.5000\n",
      "Epoch 293/500\n",
      "Epoch 00293: val_loss did not improve\n",
      " - 3s - loss: 0.8764 - acc: 0.5556 - val_loss: 0.9095 - val_acc: 0.6667\n",
      "Epoch 294/500\n",
      "Epoch 00294: val_loss did not improve\n",
      " - 3s - loss: 0.8728 - acc: 0.5556 - val_loss: 0.9000 - val_acc: 0.6667\n",
      "Epoch 295/500\n",
      "Epoch 00295: val_loss did not improve\n",
      " - 3s - loss: 0.8677 - acc: 0.5556 - val_loss: 0.8951 - val_acc: 0.6667\n",
      "Epoch 296/500\n",
      "Epoch 00296: val_loss did not improve\n",
      " - 3s - loss: 0.8555 - acc: 0.6667 - val_loss: 0.8898 - val_acc: 0.6667\n",
      "Epoch 297/500\n",
      "Epoch 00297: val_loss did not improve\n",
      " - 3s - loss: 0.8316 - acc: 0.5556 - val_loss: 0.8858 - val_acc: 0.5000\n",
      "Epoch 298/500\n",
      "Epoch 00298: val_loss did not improve\n",
      " - 3s - loss: 0.7949 - acc: 0.6667 - val_loss: 0.8949 - val_acc: 0.6667\n",
      "Epoch 299/500\n",
      "Epoch 00299: val_loss did not improve\n",
      " - 3s - loss: 0.8617 - acc: 0.5556 - val_loss: 0.8891 - val_acc: 0.6667\n",
      "Epoch 300/500\n",
      "Epoch 00300: val_loss did not improve\n",
      " - 3s - loss: 0.8411 - acc: 0.6667 - val_loss: 0.8836 - val_acc: 0.6667\n",
      "Epoch 301/500\n",
      "Epoch 00301: val_loss improved from 0.87293 to 0.80918, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.8121 - acc: 0.5556 - val_loss: 0.8092 - val_acc: 0.5000\n",
      "Epoch 302/500\n",
      "Epoch 00302: val_loss did not improve\n",
      " - 3s - loss: 0.8912 - acc: 0.5556 - val_loss: 0.8354 - val_acc: 0.5000\n",
      "Epoch 303/500\n",
      "Epoch 00303: val_loss did not improve\n",
      " - 3s - loss: 0.8873 - acc: 0.5556 - val_loss: 0.8572 - val_acc: 0.5000\n",
      "Epoch 304/500\n",
      "Epoch 00304: val_loss did not improve\n",
      " - 3s - loss: 0.8833 - acc: 0.5556 - val_loss: 0.8664 - val_acc: 0.6667\n",
      "Epoch 305/500\n",
      "Epoch 00305: val_loss did not improve\n",
      " - 3s - loss: 0.8788 - acc: 0.5556 - val_loss: 0.8710 - val_acc: 0.6667\n",
      "Epoch 306/500\n",
      "Epoch 00306: val_loss did not improve\n",
      " - 3s - loss: 0.8732 - acc: 0.5556 - val_loss: 0.8751 - val_acc: 0.6667\n",
      "Epoch 307/500\n",
      "Epoch 00307: val_loss did not improve\n",
      " - 3s - loss: 0.8652 - acc: 0.5556 - val_loss: 0.8810 - val_acc: 0.6667\n",
      "Epoch 308/500\n",
      "Epoch 00308: val_loss did not improve\n",
      " - 3s - loss: 0.8038 - acc: 0.8889 - val_loss: 1.1237 - val_acc: 0.3333\n",
      "Epoch 309/500\n",
      "Epoch 00309: val_loss did not improve\n",
      " - 3s - loss: 0.9366 - acc: 0.5556 - val_loss: 0.9012 - val_acc: 0.6667\n",
      "Epoch 310/500\n",
      "Epoch 00310: val_loss did not improve\n",
      " - 3s - loss: 0.8611 - acc: 0.7778 - val_loss: 0.8866 - val_acc: 0.6667\n",
      "Epoch 311/500\n",
      "Epoch 00311: val_loss did not improve\n",
      " - 3s - loss: 0.8317 - acc: 0.8889 - val_loss: 0.8775 - val_acc: 0.6667\n",
      "Epoch 312/500\n",
      "Epoch 00312: val_loss improved from 0.80918 to 0.79146, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.7952 - acc: 0.5556 - val_loss: 0.7915 - val_acc: 0.5000\n",
      "Epoch 313/500\n",
      "Epoch 00313: val_loss did not improve\n",
      " - 3s - loss: 0.8797 - acc: 0.5556 - val_loss: 0.8085 - val_acc: 0.6667\n",
      "Epoch 314/500\n",
      "Epoch 00314: val_loss did not improve\n",
      " - 3s - loss: 0.8753 - acc: 0.5556 - val_loss: 0.8373 - val_acc: 0.6667\n",
      "Epoch 315/500\n",
      "Epoch 00315: val_loss did not improve\n",
      " - 3s - loss: 0.8705 - acc: 0.5556 - val_loss: 0.8545 - val_acc: 0.6667\n",
      "Epoch 316/500\n",
      "Epoch 00316: val_loss did not improve\n",
      " - 3s - loss: 0.8643 - acc: 0.5556 - val_loss: 0.8641 - val_acc: 0.6667\n",
      "Epoch 317/500\n",
      "Epoch 00317: val_loss did not improve\n",
      " - 3s - loss: 0.8585 - acc: 0.6667 - val_loss: 0.8687 - val_acc: 0.6667\n",
      "Epoch 318/500\n",
      "Epoch 00318: val_loss did not improve\n",
      " - 3s - loss: 0.8534 - acc: 0.6667 - val_loss: 0.8734 - val_acc: 0.6667\n",
      "Epoch 319/500\n",
      "Epoch 00319: val_loss did not improve\n",
      " - 3s - loss: 0.8305 - acc: 0.7778 - val_loss: 0.9291 - val_acc: 0.5000\n",
      "Epoch 320/500\n",
      "Epoch 00320: val_loss did not improve\n",
      " - 3s - loss: 0.8689 - acc: 0.6667 - val_loss: 0.8886 - val_acc: 0.6667\n",
      "Epoch 321/500\n",
      "Epoch 00321: val_loss did not improve\n",
      " - 3s - loss: 0.8301 - acc: 0.8889 - val_loss: 0.8715 - val_acc: 0.6667\n",
      "Epoch 322/500\n",
      "Epoch 00322: val_loss did not improve\n",
      " - 3s - loss: 0.7565 - acc: 0.6667 - val_loss: 0.8644 - val_acc: 0.6667\n",
      "Epoch 323/500\n",
      "Epoch 00323: val_loss did not improve\n",
      " - 3s - loss: 0.8362 - acc: 0.5556 - val_loss: 0.9150 - val_acc: 0.5000\n",
      "Epoch 324/500\n",
      "Epoch 00324: val_loss did not improve\n",
      " - 3s - loss: 0.8575 - acc: 0.6667 - val_loss: 0.8798 - val_acc: 0.6667\n",
      "Epoch 325/500\n",
      "Epoch 00325: val_loss did not improve\n",
      " - 3s - loss: 0.8157 - acc: 0.8889 - val_loss: 0.8681 - val_acc: 0.6667\n",
      "Epoch 326/500\n",
      "Epoch 00326: val_loss improved from 0.79146 to 0.77587, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.7622 - acc: 0.5556 - val_loss: 0.7759 - val_acc: 0.5000\n",
      "Epoch 327/500\n",
      "Epoch 00327: val_loss did not improve\n",
      " - 3s - loss: 0.8794 - acc: 0.5556 - val_loss: 0.8780 - val_acc: 0.6667\n",
      "Epoch 328/500\n",
      "Epoch 00328: val_loss did not improve\n",
      " - 3s - loss: 0.8201 - acc: 0.8889 - val_loss: 0.8663 - val_acc: 0.6667\n",
      "Epoch 329/500\n",
      "Epoch 00329: val_loss did not improve\n",
      " - 3s - loss: 0.7791 - acc: 0.6667 - val_loss: 0.8437 - val_acc: 0.6667\n",
      "Epoch 330/500\n",
      "Epoch 00330: val_loss did not improve\n",
      " - 3s - loss: 0.8578 - acc: 0.5556 - val_loss: 0.8487 - val_acc: 0.6667\n",
      "Epoch 331/500\n",
      "Epoch 00331: val_loss did not improve\n",
      " - 3s - loss: 0.8506 - acc: 0.5556 - val_loss: 0.8550 - val_acc: 0.6667\n",
      "Epoch 332/500\n",
      "Epoch 00332: val_loss did not improve\n",
      " - 3s - loss: 0.8387 - acc: 0.6667 - val_loss: 0.8640 - val_acc: 0.6667\n",
      "Epoch 333/500\n",
      "Epoch 00333: val_loss did not improve\n",
      " - 3s - loss: 0.7969 - acc: 0.7778 - val_loss: 0.9555 - val_acc: 0.5000\n",
      "Epoch 334/500\n",
      "Epoch 00334: val_loss did not improve\n",
      " - 3s - loss: 0.8697 - acc: 0.6667 - val_loss: 0.9082 - val_acc: 0.5000\n",
      "Epoch 335/500\n",
      "Epoch 00335: val_loss did not improve\n",
      " - 3s - loss: 0.8485 - acc: 0.6667 - val_loss: 0.8748 - val_acc: 0.6667\n",
      "Epoch 336/500\n",
      "Epoch 00336: val_loss did not improve\n",
      " - 3s - loss: 0.8091 - acc: 0.8889 - val_loss: 0.8621 - val_acc: 0.6667\n",
      "Epoch 337/500\n",
      "Epoch 00337: val_loss did not improve\n",
      " - 3s - loss: 0.7748 - acc: 0.5556 - val_loss: 0.8446 - val_acc: 0.6667\n",
      "Epoch 338/500\n",
      "Epoch 00338: val_loss did not improve\n",
      " - 3s - loss: 0.8479 - acc: 0.5556 - val_loss: 0.8508 - val_acc: 0.6667\n",
      "Epoch 339/500\n",
      "Epoch 00339: val_loss did not improve\n",
      " - 3s - loss: 0.8187 - acc: 0.6667 - val_loss: 0.8763 - val_acc: 0.5000\n",
      "Epoch 340/500\n",
      "Epoch 00340: val_loss did not improve\n",
      " - 3s - loss: 0.8054 - acc: 0.7778 - val_loss: 0.8609 - val_acc: 0.6667\n",
      "Epoch 341/500\n",
      "Epoch 00341: val_loss did not improve\n",
      " - 3s - loss: 0.7449 - acc: 0.7778 - val_loss: 0.8257 - val_acc: 0.6667\n",
      "Epoch 342/500\n",
      "Epoch 00342: val_loss did not improve\n",
      " - 3s - loss: 0.8508 - acc: 0.5556 - val_loss: 0.8367 - val_acc: 0.6667\n",
      "Epoch 343/500\n",
      "Epoch 00343: val_loss did not improve\n",
      " - 3s - loss: 0.8402 - acc: 0.5556 - val_loss: 0.8508 - val_acc: 0.6667\n",
      "Epoch 344/500\n",
      "Epoch 00344: val_loss did not improve\n",
      " - 3s - loss: 0.8201 - acc: 0.7778 - val_loss: 0.8633 - val_acc: 0.6667\n",
      "Epoch 345/500\n",
      "Epoch 00345: val_loss did not improve\n",
      " - 3s - loss: 0.7333 - acc: 0.8889 - val_loss: 0.8538 - val_acc: 0.6667\n",
      "Epoch 346/500\n",
      "Epoch 00346: val_loss did not improve\n",
      " - 3s - loss: 0.7216 - acc: 0.7778 - val_loss: 0.8586 - val_acc: 0.6667\n",
      "Epoch 347/500\n",
      "Epoch 00347: val_loss improved from 0.77587 to 0.75411, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.7346 - acc: 0.7778 - val_loss: 0.7541 - val_acc: 0.6667\n",
      "Epoch 348/500\n",
      "Epoch 00348: val_loss did not improve\n",
      " - 3s - loss: 0.9930 - acc: 0.4444 - val_loss: 0.8427 - val_acc: 0.6667\n",
      "Epoch 349/500\n",
      "Epoch 00349: val_loss did not improve\n",
      " - 3s - loss: 0.8270 - acc: 0.5556 - val_loss: 0.8608 - val_acc: 0.5000\n",
      "Epoch 350/500\n",
      "Epoch 00350: val_loss did not improve\n",
      " - 3s - loss: 0.7263 - acc: 0.7778 - val_loss: 0.8607 - val_acc: 0.6667\n",
      "Epoch 351/500\n",
      "Epoch 00351: val_loss did not improve\n",
      " - 3s - loss: 0.7710 - acc: 0.7778 - val_loss: 0.8462 - val_acc: 0.6667\n",
      "Epoch 352/500\n",
      "Epoch 00352: val_loss did not improve\n",
      " - 3s - loss: 0.7570 - acc: 0.6667 - val_loss: 1.1758 - val_acc: 0.3333\n",
      "Epoch 353/500\n",
      "Epoch 00353: val_loss did not improve\n",
      " - 3s - loss: 0.9375 - acc: 0.5556 - val_loss: 0.8689 - val_acc: 0.6667\n",
      "Epoch 354/500\n",
      "Epoch 00354: val_loss did not improve\n",
      " - 3s - loss: 0.7947 - acc: 0.8889 - val_loss: 0.8524 - val_acc: 0.6667\n",
      "Epoch 355/500\n",
      "Epoch 00355: val_loss did not improve\n",
      " - 3s - loss: 0.7547 - acc: 0.7778 - val_loss: 0.8355 - val_acc: 0.6667\n",
      "Epoch 356/500\n",
      "Epoch 00356: val_loss did not improve\n",
      " - 3s - loss: 0.8200 - acc: 0.4444 - val_loss: 0.8560 - val_acc: 0.5000\n",
      "Epoch 357/500\n",
      "Epoch 00357: val_loss did not improve\n",
      " - 3s - loss: 0.7180 - acc: 0.7778 - val_loss: 0.8484 - val_acc: 0.6667\n",
      "Epoch 358/500\n",
      "Epoch 00358: val_loss did not improve\n",
      " - 3s - loss: 0.7033 - acc: 0.8889 - val_loss: 0.8410 - val_acc: 0.6667\n",
      "Epoch 359/500\n",
      "Epoch 00359: val_loss did not improve\n",
      " - 3s - loss: 0.7174 - acc: 0.7778 - val_loss: 0.9011 - val_acc: 0.5000\n",
      "Epoch 360/500\n",
      "Epoch 00360: val_loss did not improve\n",
      " - 3s - loss: 0.8360 - acc: 0.6667 - val_loss: 0.8575 - val_acc: 0.6667\n",
      "Epoch 361/500\n",
      "Epoch 00361: val_loss did not improve\n",
      " - 3s - loss: 0.7780 - acc: 0.7778 - val_loss: 0.8455 - val_acc: 0.6667\n",
      "Epoch 362/500\n",
      "Epoch 00362: val_loss did not improve\n",
      " - 3s - loss: 0.7459 - acc: 0.7778 - val_loss: 0.8246 - val_acc: 0.6667\n",
      "Epoch 363/500\n",
      "Epoch 00363: val_loss did not improve\n",
      " - 3s - loss: 0.8236 - acc: 0.4444 - val_loss: 0.8518 - val_acc: 0.5000\n",
      "Epoch 364/500\n",
      "Epoch 00364: val_loss did not improve\n",
      " - 3s - loss: 0.7837 - acc: 0.6667 - val_loss: 0.8832 - val_acc: 0.6667\n",
      "Epoch 365/500\n",
      "Epoch 00365: val_loss did not improve\n",
      " - 3s - loss: 0.8216 - acc: 0.5556 - val_loss: 0.8472 - val_acc: 0.6667\n",
      "Epoch 366/500\n",
      "Epoch 00366: val_loss did not improve\n",
      " - 3s - loss: 0.7269 - acc: 0.7778 - val_loss: 0.7943 - val_acc: 0.6667\n",
      "Epoch 367/500\n",
      "Epoch 00367: val_loss did not improve\n",
      " - 3s - loss: 0.8382 - acc: 0.5556 - val_loss: 0.8091 - val_acc: 0.6667\n",
      "Epoch 368/500\n",
      "Epoch 00368: val_loss did not improve\n",
      " - 3s - loss: 0.8300 - acc: 0.5556 - val_loss: 0.8240 - val_acc: 0.6667\n",
      "Epoch 369/500\n",
      "Epoch 00369: val_loss did not improve\n",
      " - 3s - loss: 0.8023 - acc: 0.6667 - val_loss: 0.8473 - val_acc: 0.5000\n",
      "Epoch 370/500\n",
      "Epoch 00370: val_loss did not improve\n",
      " - 3s - loss: 0.7646 - acc: 0.6667 - val_loss: 0.8922 - val_acc: 0.5000\n",
      "Epoch 371/500\n",
      "Epoch 00371: val_loss did not improve\n",
      " - 3s - loss: 0.8146 - acc: 0.6667 - val_loss: 0.8430 - val_acc: 0.6667\n",
      "Epoch 372/500\n",
      "Epoch 00372: val_loss did not improve\n",
      " - 3s - loss: 0.7201 - acc: 0.7778 - val_loss: 0.8030 - val_acc: 0.6667\n",
      "Epoch 373/500\n",
      "Epoch 00373: val_loss did not improve\n",
      " - 3s - loss: 0.8288 - acc: 0.5556 - val_loss: 0.8165 - val_acc: 0.6667\n",
      "Epoch 374/500\n",
      "Epoch 00374: val_loss did not improve\n",
      " - 3s - loss: 0.8050 - acc: 0.6667 - val_loss: 0.8510 - val_acc: 0.5000\n",
      "Epoch 375/500\n",
      "Epoch 00375: val_loss did not improve\n",
      " - 3s - loss: 0.7593 - acc: 0.7778 - val_loss: 0.8724 - val_acc: 0.5000\n",
      "Epoch 376/500\n",
      "Epoch 00376: val_loss did not improve\n",
      " - 3s - loss: 0.7720 - acc: 0.7778 - val_loss: 0.8399 - val_acc: 0.6667\n",
      "Epoch 377/500\n",
      "Epoch 00377: val_loss did not improve\n",
      " - 3s - loss: 0.6962 - acc: 0.8889 - val_loss: 0.8256 - val_acc: 0.6667\n",
      "Epoch 378/500\n",
      "Epoch 00378: val_loss did not improve\n",
      " - 3s - loss: 0.7680 - acc: 0.6667 - val_loss: 0.9364 - val_acc: 0.5000\n",
      "Epoch 379/500\n",
      "Epoch 00379: val_loss did not improve\n",
      " - 3s - loss: 0.8470 - acc: 0.5556 - val_loss: 0.8707 - val_acc: 0.6667\n",
      "Epoch 380/500\n",
      "Epoch 00380: val_loss did not improve\n",
      " - 3s - loss: 0.7916 - acc: 0.6667 - val_loss: 0.8544 - val_acc: 0.5000\n",
      "Epoch 381/500\n",
      "Epoch 00381: val_loss did not improve\n",
      " - 3s - loss: 0.7222 - acc: 0.7778 - val_loss: 0.8073 - val_acc: 0.6667\n",
      "Epoch 382/500\n",
      "Epoch 00382: val_loss did not improve\n",
      " - 3s - loss: 0.8226 - acc: 0.5556 - val_loss: 0.8174 - val_acc: 0.6667\n",
      "Epoch 383/500\n",
      "Epoch 00383: val_loss did not improve\n",
      " - 3s - loss: 0.7858 - acc: 0.6667 - val_loss: 0.8552 - val_acc: 0.6667\n",
      "Epoch 384/500\n",
      "Epoch 00384: val_loss did not improve\n",
      " - 3s - loss: 0.7578 - acc: 0.6667 - val_loss: 0.8255 - val_acc: 0.6667\n",
      "Epoch 385/500\n",
      "Epoch 00385: val_loss did not improve\n",
      " - 3s - loss: 0.7447 - acc: 0.8889 - val_loss: 0.8747 - val_acc: 0.5000\n",
      "Epoch 386/500\n",
      "Epoch 00386: val_loss did not improve\n",
      " - 3s - loss: 0.7747 - acc: 0.7778 - val_loss: 0.8358 - val_acc: 0.6667\n",
      "Epoch 387/500\n",
      "Epoch 00387: val_loss did not improve\n",
      " - 3s - loss: 0.7027 - acc: 0.7778 - val_loss: 0.8028 - val_acc: 0.6667\n",
      "Epoch 388/500\n",
      "Epoch 00388: val_loss did not improve\n",
      " - 3s - loss: 0.8075 - acc: 0.4444 - val_loss: 0.8453 - val_acc: 0.5000\n",
      "Epoch 389/500\n",
      "Epoch 00389: val_loss did not improve\n",
      " - 3s - loss: 0.7325 - acc: 0.7778 - val_loss: 0.8677 - val_acc: 0.5000\n",
      "Epoch 390/500\n",
      "Epoch 00390: val_loss did not improve\n",
      " - 3s - loss: 0.7579 - acc: 0.7778 - val_loss: 0.8290 - val_acc: 0.6667\n",
      "Epoch 391/500\n",
      "Epoch 00391: val_loss did not improve\n",
      " - 3s - loss: 0.6737 - acc: 0.8889 - val_loss: 0.8282 - val_acc: 0.6667\n",
      "Epoch 392/500\n",
      "Epoch 00392: val_loss did not improve\n",
      " - 3s - loss: 0.6671 - acc: 0.7778 - val_loss: 0.8252 - val_acc: 0.6667\n",
      "Epoch 393/500\n",
      "Epoch 00393: val_loss did not improve\n",
      " - 3s - loss: 0.6628 - acc: 0.8889 - val_loss: 0.8327 - val_acc: 0.6667\n",
      "Epoch 394/500\n",
      "Epoch 00394: val_loss did not improve\n",
      " - 3s - loss: 0.6648 - acc: 0.7778 - val_loss: 0.8120 - val_acc: 0.6667\n",
      "Epoch 395/500\n",
      "Epoch 00395: val_loss did not improve\n",
      " - 3s - loss: 0.8037 - acc: 0.5556 - val_loss: 0.8420 - val_acc: 0.6667\n",
      "Epoch 396/500\n",
      "Epoch 00396: val_loss did not improve\n",
      " - 3s - loss: 0.7378 - acc: 0.7778 - val_loss: 0.8290 - val_acc: 0.6667\n",
      "Epoch 397/500\n",
      "Epoch 00397: val_loss improved from 0.75411 to 0.71399, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.6777 - acc: 0.7778 - val_loss: 0.7140 - val_acc: 0.6667\n",
      "Epoch 398/500\n",
      "Epoch 00398: val_loss did not improve\n",
      " - 3s - loss: 0.8294 - acc: 0.5556 - val_loss: 0.8119 - val_acc: 0.6667\n",
      "Epoch 399/500\n",
      "Epoch 00399: val_loss did not improve\n",
      " - 3s - loss: 0.7393 - acc: 0.6667 - val_loss: 0.9814 - val_acc: 0.5000\n",
      "Epoch 400/500\n",
      "Epoch 00400: val_loss did not improve\n",
      " - 3s - loss: 0.8687 - acc: 0.5556 - val_loss: 0.9264 - val_acc: 0.3333\n",
      "Epoch 401/500\n",
      "Epoch 00401: val_loss did not improve\n",
      " - 3s - loss: 0.8194 - acc: 0.6667 - val_loss: 0.8364 - val_acc: 0.6667\n",
      "Epoch 402/500\n",
      "Epoch 00402: val_loss did not improve\n",
      " - 3s - loss: 0.7326 - acc: 0.7778 - val_loss: 0.8276 - val_acc: 0.6667\n",
      "Epoch 403/500\n",
      "Epoch 00403: val_loss did not improve\n",
      " - 3s - loss: 0.6970 - acc: 0.7778 - val_loss: 0.7987 - val_acc: 0.6667\n",
      "Epoch 404/500\n",
      "Epoch 00404: val_loss did not improve\n",
      " - 3s - loss: 0.7851 - acc: 0.5556 - val_loss: 0.8331 - val_acc: 0.6667\n",
      "Epoch 405/500\n",
      "Epoch 00405: val_loss did not improve\n",
      " - 3s - loss: 0.7236 - acc: 0.6667 - val_loss: 0.8332 - val_acc: 0.5000\n",
      "Epoch 406/500\n",
      "Epoch 00406: val_loss did not improve\n",
      " - 3s - loss: 0.7214 - acc: 0.7778 - val_loss: 0.8494 - val_acc: 0.6667\n",
      "Epoch 407/500\n",
      "Epoch 00407: val_loss did not improve\n",
      " - 3s - loss: 0.7351 - acc: 0.6667 - val_loss: 0.8194 - val_acc: 0.6667\n",
      "Epoch 408/500\n",
      "Epoch 00408: val_loss did not improve\n",
      " - 3s - loss: 0.6717 - acc: 0.6667 - val_loss: 0.7940 - val_acc: 0.6667\n",
      "Epoch 409/500\n",
      "Epoch 00409: val_loss did not improve\n",
      " - 3s - loss: 0.7776 - acc: 0.6667 - val_loss: 0.8262 - val_acc: 0.6667\n",
      "Epoch 410/500\n",
      "Epoch 00410: val_loss did not improve\n",
      " - 3s - loss: 0.7031 - acc: 0.6667 - val_loss: 0.8395 - val_acc: 0.5000\n",
      "Epoch 411/500\n",
      "Epoch 00411: val_loss did not improve\n",
      " - 3s - loss: 0.6755 - acc: 0.7778 - val_loss: 0.8109 - val_acc: 0.6667\n",
      "Epoch 412/500\n",
      "Epoch 00412: val_loss did not improve\n",
      " - 3s - loss: 0.6398 - acc: 0.7778 - val_loss: 0.8180 - val_acc: 0.6667\n",
      "Epoch 413/500\n",
      "Epoch 00413: val_loss did not improve\n",
      " - 3s - loss: 0.6345 - acc: 0.8889 - val_loss: 0.8095 - val_acc: 0.6667\n",
      "Epoch 414/500\n",
      "Epoch 00414: val_loss did not improve\n",
      " - 3s - loss: 0.6391 - acc: 0.7778 - val_loss: 0.8485 - val_acc: 0.5000\n",
      "Epoch 415/500\n",
      "Epoch 00415: val_loss improved from 0.71399 to 0.68664, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.6728 - acc: 0.7778 - val_loss: 0.6866 - val_acc: 0.6667\n",
      "Epoch 416/500\n",
      "Epoch 00416: val_loss did not improve\n",
      " - 3s - loss: 0.8199 - acc: 0.5556 - val_loss: 0.6967 - val_acc: 0.6667\n",
      "Epoch 417/500\n",
      "Epoch 00417: val_loss did not improve\n",
      " - 3s - loss: 0.8160 - acc: 0.5556 - val_loss: 0.7182 - val_acc: 0.6667\n",
      "Epoch 418/500\n",
      "Epoch 00418: val_loss did not improve\n",
      " - 3s - loss: 0.8130 - acc: 0.5556 - val_loss: 0.7507 - val_acc: 0.6667\n",
      "Epoch 419/500\n",
      "Epoch 00419: val_loss did not improve\n",
      " - 3s - loss: 0.8083 - acc: 0.5556 - val_loss: 0.7747 - val_acc: 0.6667\n",
      "Epoch 420/500\n",
      "Epoch 00420: val_loss did not improve\n",
      " - 3s - loss: 0.7982 - acc: 0.4444 - val_loss: 0.8103 - val_acc: 0.6667\n",
      "Epoch 421/500\n",
      "Epoch 00421: val_loss did not improve\n",
      " - 3s - loss: 0.7540 - acc: 0.5556 - val_loss: 0.9173 - val_acc: 0.5000\n",
      "Epoch 422/500\n",
      "Epoch 00422: val_loss did not improve\n",
      " - 3s - loss: 0.8203 - acc: 0.5556 - val_loss: 0.8292 - val_acc: 0.6667\n",
      "Epoch 423/500\n",
      "Epoch 00423: val_loss did not improve\n",
      " - 3s - loss: 0.6949 - acc: 0.6667 - val_loss: 0.8360 - val_acc: 0.5000\n",
      "Epoch 424/500\n",
      "Epoch 00424: val_loss did not improve\n",
      " - 3s - loss: 0.6960 - acc: 0.7778 - val_loss: 0.9616 - val_acc: 0.5000\n",
      "Epoch 425/500\n",
      "Epoch 00425: val_loss did not improve\n",
      " - 3s - loss: 0.8456 - acc: 0.5556 - val_loss: 0.8642 - val_acc: 0.5000\n",
      "Epoch 426/500\n",
      "Epoch 00426: val_loss did not improve\n",
      " - 3s - loss: 0.7528 - acc: 0.7778 - val_loss: 0.8137 - val_acc: 0.6667\n",
      "Epoch 427/500\n",
      "Epoch 00427: val_loss did not improve\n",
      " - 3s - loss: 0.6453 - acc: 0.6667 - val_loss: 0.8089 - val_acc: 0.6667\n",
      "Epoch 428/500\n",
      "Epoch 00428: val_loss did not improve\n",
      " - 3s - loss: 0.6287 - acc: 0.8889 - val_loss: 0.8434 - val_acc: 0.5000\n",
      "Epoch 429/500\n",
      "Epoch 00429: val_loss did not improve\n",
      " - 3s - loss: 0.6703 - acc: 0.7778 - val_loss: 0.7314 - val_acc: 0.6667\n",
      "Epoch 430/500\n",
      "Epoch 00430: val_loss did not improve\n",
      " - 3s - loss: 0.8263 - acc: 0.5556 - val_loss: 0.7613 - val_acc: 0.6667\n",
      "Epoch 431/500\n",
      "Epoch 00431: val_loss did not improve\n",
      " - 3s - loss: 0.8114 - acc: 0.5556 - val_loss: 0.7792 - val_acc: 0.6667\n",
      "Epoch 432/500\n",
      "Epoch 00432: val_loss did not improve\n",
      " - 3s - loss: 0.7769 - acc: 0.6667 - val_loss: 0.8205 - val_acc: 0.5000\n",
      "Epoch 433/500\n",
      "Epoch 00433: val_loss did not improve\n",
      " - 3s - loss: 0.7044 - acc: 0.6667 - val_loss: 0.8943 - val_acc: 0.5000\n",
      "Epoch 434/500\n",
      "Epoch 00434: val_loss did not improve\n",
      " - 3s - loss: 0.7800 - acc: 0.6667 - val_loss: 0.8088 - val_acc: 0.6667\n",
      "Epoch 435/500\n",
      "Epoch 00435: val_loss did not improve\n",
      " - 3s - loss: 0.6170 - acc: 0.8889 - val_loss: 0.8146 - val_acc: 0.6667\n",
      "Epoch 436/500\n",
      "Epoch 00436: val_loss did not improve\n",
      " - 3s - loss: 0.6129 - acc: 0.7778 - val_loss: 0.8078 - val_acc: 0.6667\n",
      "Epoch 437/500\n",
      "Epoch 00437: val_loss did not improve\n",
      " - 3s - loss: 0.6126 - acc: 0.8889 - val_loss: 0.8254 - val_acc: 0.6667\n",
      "Epoch 438/500\n",
      "Epoch 00438: val_loss did not improve\n",
      " - 3s - loss: 0.6209 - acc: 0.7778 - val_loss: 0.8169 - val_acc: 0.5000\n",
      "Epoch 439/500\n",
      "Epoch 00439: val_loss did not improve\n",
      " - 3s - loss: 0.6577 - acc: 0.6667 - val_loss: 1.0071 - val_acc: 0.5000\n",
      "Epoch 440/500\n",
      "Epoch 00440: val_loss did not improve\n",
      " - 3s - loss: 0.8465 - acc: 0.5556 - val_loss: 0.9815 - val_acc: 0.5000\n",
      "Epoch 441/500\n",
      "Epoch 00441: val_loss did not improve\n",
      " - 3s - loss: 0.8301 - acc: 0.6667 - val_loss: 0.9404 - val_acc: 0.5000\n",
      "Epoch 442/500\n",
      "Epoch 00442: val_loss did not improve\n",
      " - 3s - loss: 0.8060 - acc: 0.7778 - val_loss: 0.8426 - val_acc: 0.6667\n",
      "Epoch 443/500\n",
      "Epoch 00443: val_loss did not improve\n",
      " - 3s - loss: 0.7549 - acc: 0.7778 - val_loss: 0.7993 - val_acc: 0.6667\n",
      "Epoch 444/500\n",
      "Epoch 00444: val_loss did not improve\n",
      " - 3s - loss: 0.6100 - acc: 0.8889 - val_loss: 0.8400 - val_acc: 0.5000\n",
      "Epoch 445/500\n",
      "Epoch 00445: val_loss did not improve\n",
      " - 3s - loss: 0.6493 - acc: 0.7778 - val_loss: 0.7239 - val_acc: 0.6667\n",
      "Epoch 446/500\n",
      "Epoch 00446: val_loss did not improve\n",
      " - 3s - loss: 0.8085 - acc: 0.5556 - val_loss: 0.7475 - val_acc: 0.6667\n",
      "Epoch 447/500\n",
      "Epoch 00447: val_loss did not improve\n",
      " - 3s - loss: 0.7796 - acc: 0.6667 - val_loss: 0.8126 - val_acc: 0.5000\n",
      "Epoch 448/500\n",
      "Epoch 00448: val_loss did not improve\n",
      " - 3s - loss: 0.7497 - acc: 0.6667 - val_loss: 0.8181 - val_acc: 0.6667\n",
      "Epoch 449/500\n",
      "Epoch 00449: val_loss did not improve\n",
      " - 3s - loss: 0.6730 - acc: 0.6667 - val_loss: 0.7865 - val_acc: 0.6667\n",
      "Epoch 450/500\n",
      "Epoch 00450: val_loss did not improve\n",
      " - 3s - loss: 0.6920 - acc: 0.7778 - val_loss: 0.9777 - val_acc: 0.5000\n",
      "Epoch 451/500\n",
      "Epoch 00451: val_loss did not improve\n",
      " - 3s - loss: 0.8487 - acc: 0.5556 - val_loss: 0.8839 - val_acc: 0.6667\n",
      "Epoch 452/500\n",
      "Epoch 00452: val_loss did not improve\n",
      " - 3s - loss: 0.7773 - acc: 0.5556 - val_loss: 0.8026 - val_acc: 0.6667\n",
      "Epoch 453/500\n",
      "Epoch 00453: val_loss did not improve\n",
      " - 3s - loss: 0.6009 - acc: 0.8889 - val_loss: 0.7914 - val_acc: 0.6667\n",
      "Epoch 454/500\n",
      "Epoch 00454: val_loss did not improve\n",
      " - 3s - loss: 0.6023 - acc: 0.7778 - val_loss: 0.7981 - val_acc: 0.6667\n",
      "Epoch 455/500\n",
      "Epoch 00455: val_loss did not improve\n",
      " - 3s - loss: 0.5934 - acc: 0.7778 - val_loss: 0.8000 - val_acc: 0.6667\n",
      "Epoch 456/500\n",
      "Epoch 00456: val_loss did not improve\n",
      " - 3s - loss: 0.5898 - acc: 0.8889 - val_loss: 0.7993 - val_acc: 0.6667\n",
      "Epoch 457/500\n",
      "Epoch 00457: val_loss did not improve\n",
      " - 3s - loss: 0.5871 - acc: 0.8889 - val_loss: 0.8005 - val_acc: 0.6667\n",
      "Epoch 458/500\n",
      "Epoch 00458: val_loss did not improve\n",
      " - 3s - loss: 0.5846 - acc: 0.8889 - val_loss: 0.7982 - val_acc: 0.6667\n",
      "Epoch 459/500\n",
      "Epoch 00459: val_loss did not improve\n",
      " - 3s - loss: 0.5827 - acc: 0.7778 - val_loss: 0.8020 - val_acc: 0.6667\n",
      "Epoch 460/500\n",
      "Epoch 00460: val_loss did not improve\n",
      " - 3s - loss: 0.5818 - acc: 0.8889 - val_loss: 0.7842 - val_acc: 0.6667\n",
      "Epoch 461/500\n",
      "Epoch 00461: val_loss did not improve\n",
      " - 3s - loss: 0.6263 - acc: 0.7778 - val_loss: 1.2840 - val_acc: 0.3333\n",
      "Epoch 462/500\n",
      "Epoch 00462: val_loss did not improve\n",
      " - 3s - loss: 0.9596 - acc: 0.5556 - val_loss: 0.7843 - val_acc: 0.6667\n",
      "Epoch 463/500\n",
      "Epoch 00463: val_loss did not improve\n",
      " - 3s - loss: 0.5940 - acc: 0.7778 - val_loss: 0.7914 - val_acc: 0.6667\n",
      "Epoch 464/500\n",
      "Epoch 00464: val_loss did not improve\n",
      " - 3s - loss: 0.5810 - acc: 0.7778 - val_loss: 0.7937 - val_acc: 0.6667\n",
      "Epoch 465/500\n",
      "Epoch 00465: val_loss did not improve\n",
      " - 3s - loss: 0.5768 - acc: 0.7778 - val_loss: 0.7959 - val_acc: 0.6667\n",
      "Epoch 466/500\n",
      "Epoch 00466: val_loss did not improve\n",
      " - 3s - loss: 0.5730 - acc: 0.7778 - val_loss: 0.7979 - val_acc: 0.6667\n",
      "Epoch 467/500\n",
      "Epoch 00467: val_loss did not improve\n",
      " - 3s - loss: 0.5702 - acc: 0.8889 - val_loss: 0.7979 - val_acc: 0.6667\n",
      "Epoch 468/500\n",
      "Epoch 00468: val_loss did not improve\n",
      " - 3s - loss: 0.5682 - acc: 0.8889 - val_loss: 0.7982 - val_acc: 0.6667\n",
      "Epoch 469/500\n",
      "Epoch 00469: val_loss did not improve\n",
      " - 3s - loss: 0.5663 - acc: 0.8889 - val_loss: 0.7982 - val_acc: 0.6667\n",
      "Epoch 470/500\n",
      "Epoch 00470: val_loss did not improve\n",
      " - 3s - loss: 0.5644 - acc: 0.8889 - val_loss: 0.7988 - val_acc: 0.6667\n",
      "Epoch 471/500\n",
      "Epoch 00471: val_loss did not improve\n",
      " - 3s - loss: 0.5625 - acc: 0.8889 - val_loss: 0.7982 - val_acc: 0.6667\n",
      "Epoch 472/500\n",
      "Epoch 00472: val_loss did not improve\n",
      " - 3s - loss: 0.5608 - acc: 0.7778 - val_loss: 0.8003 - val_acc: 0.6667\n",
      "Epoch 473/500\n",
      "Epoch 00473: val_loss did not improve\n",
      " - 3s - loss: 0.5615 - acc: 0.8889 - val_loss: 0.7833 - val_acc: 0.6667\n",
      "Epoch 474/500\n",
      "Epoch 00474: val_loss did not improve\n",
      " - 3s - loss: 0.6008 - acc: 0.7778 - val_loss: 1.0222 - val_acc: 0.5000\n",
      "Epoch 475/500\n",
      "Epoch 00475: val_loss did not improve\n",
      " - 3s - loss: 0.8293 - acc: 0.6667 - val_loss: 0.9944 - val_acc: 0.5000\n",
      "Epoch 476/500\n",
      "Epoch 00476: val_loss did not improve\n",
      " - 3s - loss: 0.8058 - acc: 0.7778 - val_loss: 0.9191 - val_acc: 0.5000\n",
      "Epoch 477/500\n",
      "Epoch 00477: val_loss did not improve\n",
      " - 3s - loss: 0.7843 - acc: 0.7778 - val_loss: 0.8101 - val_acc: 0.6667\n",
      "Epoch 478/500\n",
      "Epoch 00478: val_loss did not improve\n",
      " - 3s - loss: 0.7276 - acc: 0.7778 - val_loss: 0.7838 - val_acc: 0.6667\n",
      "Epoch 479/500\n",
      "Epoch 00479: val_loss did not improve\n",
      " - 3s - loss: 0.5708 - acc: 0.8889 - val_loss: 0.7821 - val_acc: 0.6667\n",
      "Epoch 480/500\n",
      "Epoch 00480: val_loss did not improve\n",
      " - 3s - loss: 0.5685 - acc: 0.8889 - val_loss: 0.8324 - val_acc: 0.5000\n",
      "Epoch 481/500\n",
      "Epoch 00481: val_loss did not improve\n",
      " - 3s - loss: 0.5917 - acc: 0.7778 - val_loss: 0.7861 - val_acc: 0.6667\n",
      "Epoch 482/500\n",
      "Epoch 00482: val_loss did not improve\n",
      " - 3s - loss: 0.5581 - acc: 0.7778 - val_loss: 0.7897 - val_acc: 0.6667\n",
      "Epoch 483/500\n",
      "Epoch 00483: val_loss did not improve\n",
      " - 3s - loss: 0.5527 - acc: 0.8889 - val_loss: 0.7914 - val_acc: 0.6667\n",
      "Epoch 484/500\n",
      "Epoch 00484: val_loss did not improve\n",
      " - 3s - loss: 0.5518 - acc: 0.8889 - val_loss: 0.7812 - val_acc: 0.6667\n",
      "Epoch 485/500\n",
      "Epoch 00485: val_loss did not improve\n",
      " - 3s - loss: 0.5609 - acc: 0.7778 - val_loss: 0.7855 - val_acc: 0.6667\n",
      "Epoch 486/500\n",
      "Epoch 00486: val_loss did not improve\n",
      " - 3s - loss: 0.5717 - acc: 0.7778 - val_loss: 0.7992 - val_acc: 0.6667\n",
      "Epoch 487/500\n",
      "Epoch 00487: val_loss did not improve\n",
      " - 3s - loss: 0.6736 - acc: 0.6667 - val_loss: 0.6890 - val_acc: 0.5000\n",
      "Epoch 488/500\n",
      "Epoch 00488: val_loss improved from 0.68664 to 0.62070, saving model to model-1-best.hdf5\n",
      " - 3s - loss: 0.8044 - acc: 0.5556 - val_loss: 0.6207 - val_acc: 0.6667\n",
      "Epoch 489/500\n",
      "Epoch 00489: val_loss did not improve\n",
      " - 3s - loss: 0.8386 - acc: 0.5556 - val_loss: 0.6243 - val_acc: 0.6667\n",
      "Epoch 490/500\n",
      "Epoch 00490: val_loss did not improve\n",
      " - 3s - loss: 0.8337 - acc: 0.5556 - val_loss: 0.6562 - val_acc: 0.6667\n",
      "Epoch 491/500\n",
      "Epoch 00491: val_loss did not improve\n",
      " - 3s - loss: 0.8155 - acc: 0.5556 - val_loss: 0.7142 - val_acc: 0.6667\n",
      "Epoch 492/500\n",
      "Epoch 00492: val_loss did not improve\n",
      " - 3s - loss: 0.7601 - acc: 0.7778 - val_loss: 0.7704 - val_acc: 0.6667\n",
      "Epoch 493/500\n",
      "Epoch 00493: val_loss did not improve\n",
      " - 3s - loss: 0.7699 - acc: 0.5556 - val_loss: 0.8082 - val_acc: 0.6667\n",
      "Epoch 494/500\n",
      "Epoch 00494: val_loss did not improve\n",
      " - 3s - loss: 0.6298 - acc: 0.6667 - val_loss: 0.7492 - val_acc: 0.6667\n",
      "Epoch 495/500\n",
      "Epoch 00495: val_loss did not improve\n",
      " - 3s - loss: 0.7514 - acc: 0.6667 - val_loss: 0.7768 - val_acc: 0.6667\n",
      "Epoch 496/500\n",
      "Epoch 00496: val_loss did not improve\n",
      " - 3s - loss: 0.6863 - acc: 0.6667 - val_loss: 1.0237 - val_acc: 0.5000\n",
      "Epoch 497/500\n",
      "Epoch 00497: val_loss did not improve\n",
      " - 3s - loss: 0.8591 - acc: 0.5556 - val_loss: 0.8432 - val_acc: 0.6667\n",
      "Epoch 498/500\n",
      "Epoch 00498: val_loss did not improve\n",
      " - 3s - loss: 0.7510 - acc: 0.6667 - val_loss: 0.7997 - val_acc: 0.6667\n",
      "Epoch 499/500\n",
      "Epoch 00499: val_loss did not improve\n",
      " - 3s - loss: 0.6494 - acc: 0.6667 - val_loss: 0.7645 - val_acc: 0.6667\n",
      "Epoch 500/500\n",
      "Epoch 00500: val_loss did not improve\n",
      " - 3s - loss: 0.7115 - acc: 0.5556 - val_loss: 0.8689 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "print('Training ------------')\n",
    "model_history=model.fit(x_train, y_train,\n",
    "          validation_data=(x_test,y_test),epochs=500,verbose=2,\n",
    "          batch_size=9,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEJCAYAAACdePCvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABAEQehR/j8PTF94ZvCv/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e15874b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot(model_history,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
